---
name: ðŸ§ª Test Execution (Complete Suite)

# Comprehensive test execution workflow covering all test types
# - Unit tests (by script ranges 0000-0099, 0100-0199, etc.)
# - Domain tests (configuration, documentation, infrastructure, security, testing, utilities)
# - Integration tests (automation-scripts, orchestration, etc.)
# - Parallel execution for optimal performance
# - Coverage reporting and artifact collection

on:
  push:
    branches: [main, develop, dev, dev-staging]
    paths:
      - 'aithercore/**'
      - 'library/automation-scripts/**'
      - 'library/tests/**'
      - '**.ps1'
      - '**.psm1'
  pull_request:
    branches: [main, develop, dev, dev-staging]
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        type: choice
        options:
          - all
          - unit
          - domain
          - integration
          - quick
        default: 'all'
      coverage:
        description: 'Generate coverage report'
        type: boolean
        default: false

permissions:
  contents: read
  checks: write
  pull-requests: write

# Allow parallel execution across different test suites
concurrency:
  group: test-execution-${{ github.ref }}
  cancel-in-progress: true

env:
  AITHERZERO_CI: true
  AITHERZERO_NONINTERACTIVE: true
  AITHERZERO_SUPPRESS_BANNER: true

jobs:
  # Prepare test matrix and metadata
  prepare:
    name: ðŸŽ¯ Prepare Test Matrix
    if: github.actor != 'github-actions[bot]'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      unit-ranges: ${{ steps.matrix.outputs.unit-ranges }}
      domain-modules: ${{ steps.matrix.outputs.domain-modules }}
      integration-suites: ${{ steps.matrix.outputs.integration-suites }}
      should-run-unit: ${{ steps.matrix.outputs.should-run-unit }}
      should-run-domain: ${{ steps.matrix.outputs.should-run-domain }}
      should-run-integration: ${{ steps.matrix.outputs.should-run-integration }}

    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4

      - name: ðŸ”§ Bootstrap Environment
        shell: pwsh
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: ðŸŽ¯ Generate Test Matrix
        id: matrix
        shell: pwsh
        run: |
          $testSuite = "${{ github.event.inputs.test_suite || 'all' }}"
          
          Write-Host "ðŸŽ¯ Generating test matrix for suite: $testSuite" -ForegroundColor Cyan
          
          # Define all test ranges
          $unitRanges = @(
            '0000-0099',
            '0100-0199',
            '0200-0299',
            '0300-0399',
            '0400-0499',
            '0500-0599',
            '0700-0799',
            '0800-0899',
            '0900-0999'
          )

          # Domain modules that have test coverage
          $domainModules = @(
            'configuration',
            'documentation',
            'infrastructure',
            'security',
            'testing',
            'utilities'
          )

          # Integration test suites
          $integrationSuites = @(
            'automation-scripts',
            'orchestration',
            'bootstrap',
            'modules'
          )

          # Determine which suites to run
          $runUnit = $testSuite -in @('all', 'unit', 'quick')
          $runDomain = $testSuite -in @('all', 'domain')
          $runIntegration = $testSuite -in @('all', 'integration')

          # Output as JSON arrays for matrix
          "unit-ranges=$($unitRanges | ConvertTo-Json -Compress)" >> $env:GITHUB_OUTPUT
          "domain-modules=$($domainModules | ConvertTo-Json -Compress)" >> $env:GITHUB_OUTPUT
          "integration-suites=$($integrationSuites | ConvertTo-Json -Compress)" >> $env:GITHUB_OUTPUT
          "should-run-unit=$runUnit" >> $env:GITHUB_OUTPUT
          "should-run-domain=$runDomain" >> $env:GITHUB_OUTPUT
          "should-run-integration=$runIntegration" >> $env:GITHUB_OUTPUT

          Write-Host "âœ… Test matrix generated" -ForegroundColor Green
          Write-Host "   Unit ranges: $($unitRanges.Count) (enabled: $runUnit)" -ForegroundColor Cyan
          Write-Host "   Domain modules: $($domainModules.Count) (enabled: $runDomain)" -ForegroundColor Cyan
          Write-Host "   Integration suites: $($integrationSuites.Count) (enabled: $runIntegration)" -ForegroundColor Cyan

  # Unit tests - Parallel execution by script ranges
  unit-tests:
    name: ðŸ§ª Unit [${{ matrix.range }}]
    needs: prepare
    if: needs.prepare.outputs.should-run-unit == 'True'
    runs-on: ubuntu-latest
    timeout-minutes: 10

    strategy:
      fail-fast: false
      max-parallel: 9
      matrix:
        range: ${{ fromJson(needs.prepare.outputs.unit-ranges) }}

    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4

      - name: ðŸ”§ Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: ðŸ§ª Run Unit Tests
        id: test
        shell: pwsh
        continue-on-error: true
        run: |
          Write-Host "âš¡ Running unit tests for range: ${{ matrix.range }}" -ForegroundColor Cyan

          $testPath = "./library/tests/unit/automation-scripts/${{ matrix.range }}"

          if (Test-Path $testPath) {
            $config = New-PesterConfiguration
            $config.Run.Path = $testPath
            $config.Run.Exit = $false
            $config.Run.PassThru = $true
            $config.Output.Verbosity = 'Normal'
            $config.TestResult.Enabled = $true
            $config.TestResult.OutputPath = "./library/tests/results/Unit-${{ matrix.range }}.xml"
            $config.TestResult.OutputFormat = 'NUnitXml'

            $result = Invoke-Pester -Configuration $config

            Write-Host "âœ… Tests completed for ${{ matrix.range }}" -ForegroundColor Green
            Write-Host "   Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "   Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })

            "passed=$($result.PassedCount)" >> $env:GITHUB_OUTPUT
            "failed=$($result.FailedCount)" >> $env:GITHUB_OUTPUT
            "total=$($result.TotalCount)" >> $env:GITHUB_OUTPUT

            if ($result.FailedCount -gt 0) { exit 1 }
          } else {
            Write-Host "â­ï¸  No tests found for range ${{ matrix.range }}" -ForegroundColor Yellow
            "passed=0" >> $env:GITHUB_OUTPUT
            "failed=0" >> $env:GITHUB_OUTPUT
            "total=0" >> $env:GITHUB_OUTPUT
          }

      - name: ðŸ“Š Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-${{ matrix.range }}
          path: ./library/tests/results/Unit-${{ matrix.range }}.xml
          retention-days: 7
          if-no-files-found: warn

  # Domain tests - Parallel execution by module
  domain-tests:
    name: ðŸ—ï¸ Domain [${{ matrix.module }}]
    needs: prepare
    if: needs.prepare.outputs.should-run-domain == 'True'
    runs-on: ubuntu-latest
    timeout-minutes: 10

    strategy:
      fail-fast: false
      max-parallel: 6
      matrix:
        module: ${{ fromJson(needs.prepare.outputs.domain-modules) }}

    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4

      - name: ðŸ”§ Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: ðŸ§ª Run Domain Tests
        id: test
        shell: pwsh
        continue-on-error: true
        run: |
          Write-Host "âš¡ Running domain tests for: ${{ matrix.module }}" -ForegroundColor Cyan

          $testPath = "./library/tests/domains/${{ matrix.module }}"

          if (Test-Path $testPath) {
            $config = New-PesterConfiguration
            $config.Run.Path = $testPath
            $config.Run.Exit = $false
            $config.Run.PassThru = $true
            $config.Output.Verbosity = 'Normal'
            $config.TestResult.Enabled = $true
            $config.TestResult.OutputPath = "./library/tests/results/Domain-${{ matrix.module }}.xml"
            $config.TestResult.OutputFormat = 'NUnitXml'

            $result = Invoke-Pester -Configuration $config

            Write-Host "âœ… Domain tests completed for ${{ matrix.module }}" -ForegroundColor Green
            Write-Host "   Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "   Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })

            "passed=$($result.PassedCount)" >> $env:GITHUB_OUTPUT
            "failed=$($result.FailedCount)" >> $env:GITHUB_OUTPUT
            "total=$($result.TotalCount)" >> $env:GITHUB_OUTPUT

            if ($result.FailedCount -gt 0) { exit 1 }
          } else {
            Write-Host "â­ï¸  No tests found for domain ${{ matrix.module }}" -ForegroundColor Yellow
            "passed=0" >> $env:GITHUB_OUTPUT
            "failed=0" >> $env:GITHUB_OUTPUT
            "total=0" >> $env:GITHUB_OUTPUT
          }

      - name: ðŸ“Š Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: domain-${{ matrix.module }}
          path: ./library/tests/results/Domain-${{ matrix.module }}.xml
          retention-days: 7
          if-no-files-found: warn

  # Integration tests - Parallel execution by suite
  integration-tests:
    name: ðŸ”— Integration [${{ matrix.suite }}]
    needs: prepare
    if: needs.prepare.outputs.should-run-integration == 'True'
    runs-on: ubuntu-latest
    timeout-minutes: 15

    strategy:
      fail-fast: false
      max-parallel: 4
      matrix:
        suite: ${{ fromJson(needs.prepare.outputs.integration-suites) }}

    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4

      - name: ðŸ”§ Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: ðŸ§ª Run Integration Tests
        id: test
        shell: pwsh
        continue-on-error: true
        run: |
          Write-Host "âš¡ Running integration tests for: ${{ matrix.suite }}" -ForegroundColor Cyan

          $testPath = "./library/tests/integration/${{ matrix.suite }}"
          
          # Also check for suite-specific test files in integration root
          $testFiles = @()
          if (Test-Path $testPath) {
            $testFiles += Get-ChildItem -Path $testPath -Filter "*.Tests.ps1" -Recurse
          }
          
          # Check for test files matching suite name in integration root
          $suiteName = "${{ matrix.suite }}"
          $rootTests = Get-ChildItem -Path "./library/tests/integration" -Filter "*$suiteName*.Tests.ps1" -ErrorAction SilentlyContinue
          if ($rootTests) {
            $testFiles += $rootTests
          }

          if ($testFiles.Count -gt 0) {
            $config = New-PesterConfiguration
            $config.Run.Path = if (Test-Path $testPath) { $testPath } else { "./library/tests/integration" }
            $config.Run.Exit = $false
            $config.Run.PassThru = $true
            $config.Output.Verbosity = 'Normal'
            $config.TestResult.Enabled = $true
            $config.TestResult.OutputPath = "./library/tests/results/Integration-${{ matrix.suite }}.xml"
            $config.TestResult.OutputFormat = 'NUnitXml'

            $result = Invoke-Pester -Configuration $config

            Write-Host "âœ… Integration tests completed for ${{ matrix.suite }}" -ForegroundColor Green
            Write-Host "   Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "   Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })

            "passed=$($result.PassedCount)" >> $env:GITHUB_OUTPUT
            "failed=$($result.FailedCount)" >> $env:GITHUB_OUTPUT
            "total=$($result.TotalCount)" >> $env:GITHUB_OUTPUT

            if ($result.FailedCount -gt 0) { exit 1 }
          } else {
            Write-Host "â­ï¸  No tests found for suite ${{ matrix.suite }}" -ForegroundColor Yellow
            "passed=0" >> $env:GITHUB_OUTPUT
            "failed=0" >> $env:GITHUB_OUTPUT
            "total=0" >> $env:GITHUB_OUTPUT
          }

      - name: ðŸ“Š Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-${{ matrix.suite }}
          path: ./library/tests/results/Integration-${{ matrix.suite }}.xml
          retention-days: 7
          if-no-files-found: warn

  # Coverage analysis (optional)
  coverage:
    name: ðŸ“Š Coverage Analysis
    needs: [unit-tests, domain-tests, integration-tests]
    if: |
      always() && 
      (github.event.inputs.coverage == 'true' || github.event_name == 'push')
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4

      - name: ðŸ”§ Bootstrap
        shell: pwsh
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: ðŸ“Š Generate Coverage Report
        shell: pwsh
        continue-on-error: true
        run: |
          Write-Host "ðŸ“Š Generating code coverage report..." -ForegroundColor Cyan
          
          Import-Module ./AitherZero.psd1 -Force
          
          # Run coverage generation script if available
          if (Test-Path "./library/automation-scripts/0406_Generate-Coverage.ps1") {
            & ./library/automation-scripts/0406_Generate-Coverage.ps1
          } else {
            Write-Host "âš ï¸  Coverage script not found" -ForegroundColor Yellow
          }

      - name: ðŸ“Š Upload Coverage
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: ./library/tests/coverage/**/*
          retention-days: 30
          if-no-files-found: warn

  # Consolidate results and publish summary
  summary:
    name: ðŸ“‹ Test Summary
    needs: [prepare, unit-tests, domain-tests, integration-tests]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4

      - name: ðŸ“¥ Download All Results
        uses: actions/download-artifact@v4
        with:
          path: ./artifacts

      - name: ðŸ“Š Consolidate Results
        shell: pwsh
        run: |
          Write-Host "ðŸ“Š Consolidating test results..." -ForegroundColor Cyan
          
          $totalPassed = 0
          $totalFailed = 0
          $totalTests = 0
          
          # Find all XML result files
          $results = Get-ChildItem -Path "./artifacts" -Filter "*.xml" -Recurse -ErrorAction SilentlyContinue
          
          Write-Host "Found $($results.Count) result files" -ForegroundColor Cyan
          
          foreach ($file in $results) {
            try {
              [xml]$xml = Get-Content $file.FullName
              $passed = [int]($xml.'test-results'.total - $xml.'test-results'.failures - $xml.'test-results'.errors)
              $failed = [int]($xml.'test-results'.failures + $xml.'test-results'.errors)
              $total = [int]$xml.'test-results'.total
              
              $totalPassed += $passed
              $totalFailed += $failed
              $totalTests += $total
            }
            catch {
              Write-Host "âš ï¸  Could not parse $($file.Name): $_" -ForegroundColor Yellow
            }
          }
          
          Write-Host "`nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—" -ForegroundColor Cyan
          Write-Host "â•‘           TEST EXECUTION SUMMARY                 â•‘" -ForegroundColor Cyan
          Write-Host "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" -ForegroundColor Cyan
          Write-Host ""
          Write-Host "Total Tests: $totalTests" -ForegroundColor White
          Write-Host "Passed: $totalPassed" -ForegroundColor Green
          Write-Host "Failed: $totalFailed" -ForegroundColor $(if ($totalFailed -gt 0) { 'Red' } else { 'Green' })
          Write-Host "Success Rate: $([math]::Round(($totalPassed / $totalTests) * 100, 2))%" -ForegroundColor Cyan
          Write-Host ""
          
          # Save to GitHub output
          "total=$totalTests" >> $env:GITHUB_OUTPUT
          "passed=$totalPassed" >> $env:GITHUB_OUTPUT
          "failed=$totalFailed" >> $env:GITHUB_OUTPUT

      - name: ðŸ“Š Publish Test Results
        if: always()
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          files: |
            artifacts/**/*.xml
          check_name: 'Test Results Summary'
          comment_mode: always

      - name: ðŸ’¬ Post Summary Comment
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Read all result files
            let totalPassed = 0;
            let totalFailed = 0;
            let totalTests = 0;
            
            const artifactDir = './artifacts';
            
            function walkDir(dir) {
              const files = fs.readdirSync(dir);
              for (const file of files) {
                const filePath = path.join(dir, file);
                const stat = fs.statSync(filePath);
                if (stat.isDirectory()) {
                  walkDir(filePath);
                } else if (file.endsWith('.xml')) {
                  try {
                    // Simple XML parsing for test counts
                    const content = fs.readFileSync(filePath, 'utf8');
                    const match = content.match(/total="(\d+)".*failures="(\d+)".*errors="(\d+)"/);
                    if (match) {
                      const total = parseInt(match[1]);
                      const failures = parseInt(match[2]);
                      const errors = parseInt(match[3]);
                      totalTests += total;
                      totalFailed += failures + errors;
                      totalPassed += total - failures - errors;
                    }
                  } catch (e) {
                    console.log(`Could not parse ${file}`);
                  }
                }
              }
            }
            
            if (fs.existsSync(artifactDir)) {
              walkDir(artifactDir);
            }
            
            const successRate = totalTests > 0 ? ((totalPassed / totalTests) * 100).toFixed(2) : 0;
            const status = totalFailed === 0 ? 'âœ… **ALL PASSED**' : `âš ï¸ **${totalFailed} FAILED**`;
            
            let comment = `## ðŸ§ª Test Execution Summary\n\n`;
            comment += `### ${status}\n\n`;
            comment += `| Metric | Count |\n`;
            comment += `|--------|-------|\n`;
            comment += `| **Total Tests** | ${totalTests} |\n`;
            comment += `| **Passed** | âœ… ${totalPassed} |\n`;
            comment += `| **Failed** | ${totalFailed > 0 ? 'âŒ' : 'âœ…'} ${totalFailed} |\n`;
            comment += `| **Success Rate** | ${successRate}% |\n\n`;
            
            comment += `### ðŸ“Š Test Suites\n\n`;
            comment += `- **Unit Tests**: Automation scripts by range (0000-0999)\n`;
            comment += `- **Domain Tests**: Module-specific tests (configuration, infrastructure, etc.)\n`;
            comment += `- **Integration Tests**: End-to-end workflow tests\n\n`;
            
            if (totalFailed > 0) {
              comment += `### âš ï¸ Next Steps\n\n`;
              comment += `Some tests failed. Please:\n`;
              comment += `1. Download test artifacts from this workflow run\n`;
              comment += `2. Review failed test details\n`;
              comment += `3. Fix issues and push updates\n\n`;
            }
            
            comment += `---\n`;
            comment += `*ðŸ¤– Automated by Test Execution Workflow*`;
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number
            });
            
            const existingComment = comments.find(c =>
              c.user.login === 'github-actions[bot]' &&
              c.body.includes('Test Execution Summary')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.payload.pull_request.number,
                body: comment
              });
            }

      - name: âœ… Complete
        if: always()
        shell: pwsh
        run: |
          Write-Host "`nâœ… Test execution workflow complete!" -ForegroundColor Green
          Write-Host "All test results have been collected and published." -ForegroundColor Cyan

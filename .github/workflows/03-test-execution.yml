---
name: üß™ Test Execution (Complete Suite)

# Comprehensive test execution workflow covering all test types
# - Unit tests (by script ranges 0000-0099, 0100-0199, etc.)
# - Domain tests (configuration, documentation, infrastructure, security, testing, utilities)
# - Integration tests (automation-scripts, orchestration, etc.)
# - Parallel execution for optimal performance
# - Coverage reporting and artifact collection

'on':
  # Called by master orchestrator for coordinated execution
  workflow_call:
    inputs:
      test_suite:
        description: 'Test suite to run'
        type: string
        default: 'all'
      coverage:
        description: 'Generate coverage report'
        type: boolean
        default: false
  
  # Direct triggers for standalone testing
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        type: choice
        options:
          - all
          - unit
          - domain
          - integration
          - quick
        default: 'all'
      coverage:
        description: 'Generate coverage report'
        type: boolean
        default: false

permissions:
  contents: read
  checks: write
  pull-requests: write

# Allow parallel execution across different test suites but prevent duplicates for same PR
concurrency:
  group: tests-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  AITHERZERO_CI: true
  AITHERZERO_NONINTERACTIVE: true
  AITHERZERO_SUPPRESS_BANNER: true

jobs:
  # Prepare test matrix and metadata
  prepare:
    name: üéØ Prepare Test Matrix
    if: github.actor != 'github-actions[bot]'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      unit-ranges: ${{ steps.matrix.outputs.unit-ranges }}
      domain-modules: ${{ steps.matrix.outputs.domain-modules }}
      integration-suites: ${{ steps.matrix.outputs.integration-suites }}
      should-run-unit: ${{ steps.matrix.outputs.should-run-unit }}
      should-run-domain: ${{ steps.matrix.outputs.should-run-domain }}
      should-run-integration: ${{ steps.matrix.outputs.should-run-integration }}

    steps:
      - name: üì• Checkout
        uses: actions/checkout@v4

      - name: üîß Bootstrap Environment
        shell: pwsh
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: üéØ Generate Test Matrix
        id: matrix
        shell: pwsh
        run: |
          $testSuite = "${{ github.event.inputs.test_suite || 'all' }}"
          
          Write-Host "üéØ Generating test matrix for suite: $testSuite" -ForegroundColor Cyan
          
          # Define all test ranges
          $unitRanges = @(
            '0000-0099',
            '0100-0199',
            '0200-0299',
            '0300-0399',
            '0400-0499',
            '0500-0599',
            '0700-0799',
            '0800-0899',
            '0900-0999'
          )

          # Domain modules that have test coverage
          $domainModules = @(
            'configuration',
            'documentation',
            'infrastructure',
            'security',
            'testing',
            'utilities'
          )

          # Integration test suites
          $integrationSuites = @(
            'automation-scripts',
            'orchestration',
            'bootstrap',
            'modules'
          )

          # Determine which suites to run
          $runUnit = $testSuite -in @('all', 'unit', 'quick')
          $runDomain = $testSuite -in @('all', 'domain')
          $runIntegration = $testSuite -in @('all', 'integration')

          # Output as JSON arrays for matrix
          "unit-ranges=$($unitRanges | ConvertTo-Json -Compress)" >> $env:GITHUB_OUTPUT
          "domain-modules=$($domainModules | ConvertTo-Json -Compress)" >> $env:GITHUB_OUTPUT
          "integration-suites=$($integrationSuites | ConvertTo-Json -Compress)" >> $env:GITHUB_OUTPUT
          "should-run-unit=$runUnit" >> $env:GITHUB_OUTPUT
          "should-run-domain=$runDomain" >> $env:GITHUB_OUTPUT
          "should-run-integration=$runIntegration" >> $env:GITHUB_OUTPUT

          Write-Host "‚úÖ Test matrix generated" -ForegroundColor Green
          Write-Host "   Unit ranges: $($unitRanges.Count) (enabled: $runUnit)" -ForegroundColor Cyan
          Write-Host "   Domain modules: $($domainModules.Count) (enabled: $runDomain)" -ForegroundColor Cyan
          Write-Host "   Integration suites: $($integrationSuites.Count) (enabled: $runIntegration)" -ForegroundColor Cyan

  # Unit tests - Parallel execution by script ranges
  unit-tests:
    name: üß™ Unit [${{ matrix.range }}]
    needs: prepare
    if: needs.prepare.outputs.should-run-unit == 'True'
    runs-on: ubuntu-latest
    timeout-minutes: 10

    strategy:
      fail-fast: false
      max-parallel: 9
      matrix:
        range: ${{ fromJson(needs.prepare.outputs.unit-ranges) }}

    steps:
      - name: üì• Checkout
        uses: actions/checkout@v4

      - name: üîß Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: üß™ Run Unit Tests
        id: test
        shell: pwsh
        run: |
          Write-Host "‚ö° Running unit tests for range: ${{ matrix.range }}" -ForegroundColor Cyan

          $testPath = "./library/tests/unit/automation-scripts/${{ matrix.range }}"

          if (Test-Path $testPath) {
            $config = New-PesterConfiguration
            $config.Run.Path = $testPath
            $config.Run.Exit = $false
            $config.Run.PassThru = $true
            $config.Output.Verbosity = 'Normal'
            $config.TestResult.Enabled = $true
            $config.TestResult.OutputPath = "./library/tests/results/Unit-${{ matrix.range }}.xml"
            $config.TestResult.OutputFormat = 'NUnitXml'

            $result = Invoke-Pester -Configuration $config

            Write-Host "‚úÖ Tests completed for ${{ matrix.range }}" -ForegroundColor Green
            Write-Host "   Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "   Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })

            "passed=$($result.PassedCount)" >> $env:GITHUB_OUTPUT
            "failed=$($result.FailedCount)" >> $env:GITHUB_OUTPUT
            "total=$($result.TotalCount)" >> $env:GITHUB_OUTPUT

            if ($result.FailedCount -gt 0) { 
              Write-Host "‚ùå Unit tests FAILED for ${{ matrix.range }}" -ForegroundColor Red
              exit 1 
            }
          } else {
            Write-Host "‚è≠Ô∏è  No tests found for range ${{ matrix.range }}" -ForegroundColor Yellow
            "passed=0" >> $env:GITHUB_OUTPUT
            "failed=0" >> $env:GITHUB_OUTPUT
            "total=0" >> $env:GITHUB_OUTPUT
          }

      - name: üìä Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-${{ matrix.range }}
          path: ./library/tests/results/Unit-${{ matrix.range }}.xml
          retention-days: 7
          if-no-files-found: warn

  # Domain tests - Parallel execution by module
  domain-tests:
    name: üèóÔ∏è Domain [${{ matrix.module }}]
    needs: prepare
    if: needs.prepare.outputs.should-run-domain == 'True'
    runs-on: ubuntu-latest
    timeout-minutes: 10

    strategy:
      fail-fast: false
      max-parallel: 6
      matrix:
        module: ${{ fromJson(needs.prepare.outputs.domain-modules) }}

    steps:
      - name: üì• Checkout
        uses: actions/checkout@v4

      - name: üîß Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: üß™ Run Domain Tests
        id: test
        shell: pwsh
        run: |
          Write-Host "‚ö° Running domain tests for: ${{ matrix.module }}" -ForegroundColor Cyan

          $testPath = "./library/tests/domains/${{ matrix.module }}"

          if (Test-Path $testPath) {
            $config = New-PesterConfiguration
            $config.Run.Path = $testPath
            $config.Run.Exit = $false
            $config.Run.PassThru = $true
            $config.Output.Verbosity = 'Normal'
            $config.TestResult.Enabled = $true
            $config.TestResult.OutputPath = "./library/tests/results/Domain-${{ matrix.module }}.xml"
            $config.TestResult.OutputFormat = 'NUnitXml'

            $result = Invoke-Pester -Configuration $config

            Write-Host "‚úÖ Domain tests completed for ${{ matrix.module }}" -ForegroundColor Green
            Write-Host "   Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "   Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })

            "passed=$($result.PassedCount)" >> $env:GITHUB_OUTPUT
            "failed=$($result.FailedCount)" >> $env:GITHUB_OUTPUT
            "total=$($result.TotalCount)" >> $env:GITHUB_OUTPUT

            if ($result.FailedCount -gt 0) { 
              Write-Host "‚ùå Domain tests FAILED for ${{ matrix.module }}" -ForegroundColor Red
              exit 1 
            }
          } else {
            Write-Host "‚è≠Ô∏è  No tests found for domain ${{ matrix.module }}" -ForegroundColor Yellow
            "passed=0" >> $env:GITHUB_OUTPUT
            "failed=0" >> $env:GITHUB_OUTPUT
            "total=0" >> $env:GITHUB_OUTPUT
          }

      - name: üìä Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: domain-${{ matrix.module }}
          path: ./library/tests/results/Domain-${{ matrix.module }}.xml
          retention-days: 7
          if-no-files-found: warn

  # Integration tests - Parallel execution by suite
  integration-tests:
    name: üîó Integration [${{ matrix.suite }}]
    needs: prepare
    if: needs.prepare.outputs.should-run-integration == 'True'
    runs-on: ubuntu-latest
    timeout-minutes: 15

    strategy:
      fail-fast: false
      max-parallel: 4
      matrix:
        suite: ${{ fromJson(needs.prepare.outputs.integration-suites) }}

    steps:
      - name: üì• Checkout
        uses: actions/checkout@v4

      - name: üîß Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: üß™ Run Integration Tests
        id: test
        shell: pwsh
        run: |
          Write-Host "‚ö° Running integration tests for: ${{ matrix.suite }}" -ForegroundColor Cyan

          $testPath = "./library/tests/integration/${{ matrix.suite }}"
          
          # Also check for suite-specific test files in integration root
          $testFiles = @()
          if (Test-Path $testPath) {
            $testFiles += Get-ChildItem -Path $testPath -Filter "*.Tests.ps1" -Recurse
          }
          
          # Check for test files matching suite name in integration root
          $suiteName = "${{ matrix.suite }}"
          $rootTests = Get-ChildItem -Path "./library/tests/integration" -Filter "*$suiteName*.Tests.ps1" -ErrorAction SilentlyContinue
          if ($rootTests) {
            $testFiles += $rootTests
          }

          if ($testFiles.Count -gt 0) {
            $config = New-PesterConfiguration
            $config.Run.Path = if (Test-Path $testPath) { $testPath } else { "./library/tests/integration" }
            $config.Run.Exit = $false
            $config.Run.PassThru = $true
            $config.Output.Verbosity = 'Normal'
            $config.TestResult.Enabled = $true
            $config.TestResult.OutputPath = "./library/tests/results/Integration-${{ matrix.suite }}.xml"
            $config.TestResult.OutputFormat = 'NUnitXml'

            $result = Invoke-Pester -Configuration $config

            Write-Host "‚úÖ Integration tests completed for ${{ matrix.suite }}" -ForegroundColor Green
            Write-Host "   Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "   Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })

            "passed=$($result.PassedCount)" >> $env:GITHUB_OUTPUT
            "failed=$($result.FailedCount)" >> $env:GITHUB_OUTPUT
            "total=$($result.TotalCount)" >> $env:GITHUB_OUTPUT

            if ($result.FailedCount -gt 0) { 
              Write-Host "‚ùå Integration tests FAILED for ${{ matrix.suite }}" -ForegroundColor Red
              exit 1 
            }
          } else {
            Write-Host "‚è≠Ô∏è  No tests found for suite ${{ matrix.suite }}" -ForegroundColor Yellow
            "passed=0" >> $env:GITHUB_OUTPUT
            "failed=0" >> $env:GITHUB_OUTPUT
            "total=0" >> $env:GITHUB_OUTPUT
          }

      - name: üìä Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-${{ matrix.suite }}
          path: ./library/tests/results/Integration-${{ matrix.suite }}.xml
          retention-days: 7
          if-no-files-found: warn

  # Coverage and Performance Analysis
  coverage-and-performance:
    name: üìä Coverage & Performance
    needs: [unit-tests, domain-tests, integration-tests]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: üì• Checkout
        uses: actions/checkout@v4

      - name: üîß Bootstrap
        shell: pwsh
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: üì• Download Test Results
        uses: actions/download-artifact@v4
        with:
          path: ./artifacts
        continue-on-error: true

      - name: üìä Organize Test Results for Coverage
        shell: pwsh
        run: |
          Write-Host "üì¶ Organizing test results for coverage analysis..." -ForegroundColor Cyan
          
          # Ensure directories exist
          New-Item -ItemType Directory -Path "library/tests/results" -Force | Out-Null
          
          # Copy test result files if artifacts were downloaded
          if (Test-Path "./artifacts") {
            Get-ChildItem "./artifacts" -Recurse -Filter "*.xml" | ForEach-Object {
              Copy-Item $_.FullName -Destination "library/tests/results/" -Force
              Write-Host "  ‚úì Copied: $($_.Name)" -ForegroundColor Green
            }
          }

      - name: üìä Generate Coverage Report
        id: coverage
        shell: pwsh
        continue-on-error: true
        run: |
          Write-Host "üìä Generating code coverage report..." -ForegroundColor Cyan
          
          Import-Module ./AitherZero.psd1 -Force
          
          # Run coverage generation script
          if (Test-Path "./library/automation-scripts/0406_Generate-Coverage.ps1") {
            & ./library/automation-scripts/0406_Generate-Coverage.ps1 -RunTests:$false
            if ($LASTEXITCODE -eq 0) {
              Write-Host "‚úÖ Coverage report generated successfully" -ForegroundColor Green
            } else {
              Write-Host "‚ö†Ô∏è Coverage generation completed with warnings (exit code: $LASTEXITCODE)" -ForegroundColor Yellow
            }
          } else {
            Write-Host "‚ö†Ô∏è Coverage script not found" -ForegroundColor Yellow
          }

      - name: üöÄ Collect Performance Metrics
        id: performance
        shell: pwsh
        continue-on-error: true
        run: |
          Write-Host "üöÄ Collecting performance metrics..." -ForegroundColor Cyan
          
          Import-Module ./AitherZero.psd1 -Force
          
          # Create performance metrics file
          $perfMetrics = @{
            Timestamp = Get-Date -Format "o"
            TestExecution = @{
              TotalDuration = 0
              AverageDuration = 0
              FastestTest = ""
              SlowestTest = ""
            }
            ModulePerformance = @{}
          }
          
          # Analyze test result timings if available
          $testResults = Get-ChildItem -Path "library/tests/results" -Filter "*.xml" -ErrorAction SilentlyContinue
          if ($testResults) {
            $totalTime = 0
            $testCount = 0
            
            foreach ($file in $testResults) {
              try {
                [xml]$xml = Get-Content $file.FullName
                $time = [double]$xml.'test-results'.time
                $totalTime += $time
                $testCount++
              } catch {
                # Skip files that can't be parsed
              }
            }
            
            if ($testCount -gt 0) {
              $perfMetrics.TestExecution.TotalDuration = [math]::Round($totalTime, 2)
              $perfMetrics.TestExecution.AverageDuration = [math]::Round($totalTime / $testCount, 2)
            }
          }
          
          # Run module performance profiling if available
          if (Test-Path "./library/automation-scripts/0529_Profile-ModulePerformance.ps1") {
            & ./library/automation-scripts/0529_Profile-ModulePerformance.ps1 -OutputPath "library/reports/metrics/performance-metrics.json"
          }
          
          # Save performance metrics
          $outputPath = "library/reports/metrics/performance-metrics.json"
          New-Item -ItemType Directory -Path (Split-Path $outputPath) -Force | Out-Null
          $perfMetrics | ConvertTo-Json -Depth 10 | Out-File -FilePath $outputPath -Encoding UTF8
          
          Write-Host "‚úÖ Performance metrics collected" -ForegroundColor Green
          Write-Host "  Total test duration: $($perfMetrics.TestExecution.TotalDuration)s" -ForegroundColor Cyan

      - name: üìä Upload Coverage
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: ./library/tests/coverage/**/*
          retention-days: 30
          if-no-files-found: warn
          
      - name: üöÄ Upload Performance Metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-metrics
          path: ./library/reports/metrics/performance-*.json
          retention-days: 30
          if-no-files-found: warn

  # Consolidate results and publish summary
  summary:
    name: üìã Test Summary
    needs: [prepare, unit-tests, domain-tests, integration-tests, coverage-and-performance]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: üì• Checkout
        uses: actions/checkout@v4

      - name: üì• Download All Results
        uses: actions/download-artifact@v4
        with:
          path: ./artifacts

      - name: üìä Consolidate Results
        shell: pwsh
        run: |
          Write-Host "üìä Consolidating test results..." -ForegroundColor Cyan
          
          $totalPassed = 0
          $totalFailed = 0
          $totalTests = 0
          
          # Find all XML result files
          $results = Get-ChildItem -Path "./artifacts" -Filter "*.xml" -Recurse -ErrorAction SilentlyContinue
          
          Write-Host "Found $($results.Count) result files" -ForegroundColor Cyan
          
          foreach ($file in $results) {
            try {
              [xml]$xml = Get-Content $file.FullName
              $passed = [int]($xml.'test-results'.total - $xml.'test-results'.failures - $xml.'test-results'.errors)
              $failed = [int]($xml.'test-results'.failures + $xml.'test-results'.errors)
              $total = [int]$xml.'test-results'.total
              
              $totalPassed += $passed
              $totalFailed += $failed
              $totalTests += $total
            }
            catch {
              Write-Host "‚ö†Ô∏è  Could not parse $($file.Name): $_" -ForegroundColor Yellow
            }
          }
          
          Write-Host "`n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó" -ForegroundColor Cyan
          Write-Host "‚ïë           TEST EXECUTION SUMMARY                 ‚ïë" -ForegroundColor Cyan
          Write-Host "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù" -ForegroundColor Cyan
          Write-Host ""
          Write-Host "Total Tests: $totalTests" -ForegroundColor White
          Write-Host "Passed: $totalPassed" -ForegroundColor Green
          Write-Host "Failed: $totalFailed" -ForegroundColor $(if ($totalFailed -gt 0) { 'Red' } else { 'Green' })
          Write-Host "Success Rate: $([math]::Round(($totalPassed / $totalTests) * 100, 2))%" -ForegroundColor Cyan
          Write-Host ""
          
          # Save to GitHub output
          "total=$totalTests" >> $env:GITHUB_OUTPUT
          "passed=$totalPassed" >> $env:GITHUB_OUTPUT
          "failed=$totalFailed" >> $env:GITHUB_OUTPUT

      - name: üìä Publish Test Results
        if: always()
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          files: |
            artifacts/**/*.xml
          check_name: 'Test Results Summary'
          comment_mode: off

      - name: üí¨ Post Summary Comment
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Read all result files
            let totalPassed = 0;
            let totalFailed = 0;
            let totalTests = 0;
            const results = [];  // Track individual test suite results
            
            const artifactDir = './artifacts';
            
            function walkDir(dir) {
              const files = fs.readdirSync(dir);
              for (const file of files) {
                const filePath = path.join(dir, file);
                const stat = fs.statSync(filePath);
                if (stat.isDirectory()) {
                  walkDir(filePath);
                } else if (file.endsWith('.xml')) {
                  try {
                    // Simple XML parsing for test counts
                    const content = fs.readFileSync(filePath, 'utf8');
                    const match = content.match(/total="(\d+)".*failures="(\d+)".*errors="(\d+)"/);
                    if (match) {
                      const total = parseInt(match[1]);
                      const failures = parseInt(match[2]);
                      const errors = parseInt(match[3]);
                      const passed = total - failures - errors;
                      const failed = failures + errors;
                      
                      totalTests += total;
                      totalFailed += failed;
                      totalPassed += passed;
                      
                      // Extract test suite name from file path
                      // Artifact structure: ./artifacts/unit-0000-0099/Unit-0000-0099.xml
                      // or ./artifacts/domain-configuration/Domain-configuration.xml
                      const dirName = path.basename(path.dirname(filePath));
                      results.push({
                        name: dirName,
                        passed: passed,
                        failed: failed,
                        total: total
                      });
                    }
                  } catch (e) {
                    console.log(`Could not parse ${file}`);
                  }
                }
              }
            }
            
            if (fs.existsSync(artifactDir)) {
              walkDir(artifactDir);
            }
            
            const successRate = totalTests > 0 ? ((totalPassed / totalTests) * 100).toFixed(2) : 0;
            const overallStatus = totalFailed === 0 ? '‚úÖ ALL PASSED' : `‚ùå ${totalFailed} FAILED`;
            const overallIcon = totalFailed === 0 ? '‚úÖ' : '‚ùå';
            
            // Build detailed summary with rich context
            let comment = `## üß™ Test Execution Workflow\n\n`;
            
            // Context header
            comment += `**Workflow:** \`03-test-execution.yml\`  \n`;
            comment += `**Run ID:** [${context.runId}](${context.payload.repository.html_url}/actions/runs/${context.runId})  \n`;
            comment += `**Commit:** \`${context.sha.substring(0, 7)}\`  \n`;
            comment += `**Triggered:** ${new Date().toISOString()}  \n`;
            comment += `**Event:** ${context.eventName}\n\n`;
            
            comment += `---\n\n`;
            
            // Overall status
            comment += `### ${overallIcon} Overall Status: ${overallStatus}\n\n`;
            
            // Summary table
            comment += `| Metric | Count | Percentage |\n`;
            comment += `|--------|-------|------------|\n`;
            comment += `| **Total Tests** | ${totalTests} | 100% |\n`;
            comment += `| **Passed** | ‚úÖ ${totalPassed} | ${successRate}% |\n`;
            comment += `| **Failed** | ${totalFailed > 0 ? '‚ùå' : '‚úÖ'} ${totalFailed} | ${(100 - successRate).toFixed(2)}% |\n\n`;
            
            // Detailed breakdown by test type
            comment += `### üìä Test Suite Breakdown\n\n`;
            
            // Collect test results by type
            const unitTests = [];
            const domainTests = [];
            const integrationTests = [];
            
            results.forEach(result => {
              const match = result.name.match(/^(unit|domain|integration)-(.+)$/);
              if (match) {
                const [, type, name] = match;
                const passed = parseInt(result.passed) || 0;
                const failed = parseInt(result.failed) || 0;
                const total = parseInt(result.total) || 0;
                const status = failed === 0 ? '‚úÖ' : '‚ùå';
                const item = `${status} **${name}**: ${passed}/${total} passed${failed > 0 ? ` (${failed} failed)` : ''}`;
                
                if (type === 'unit') unitTests.push(item);
                else if (type === 'domain') domainTests.push(item);
                else if (type === 'integration') integrationTests.push(item);
              }
            });
            
            if (unitTests.length > 0) {
              comment += `**üî¢ Unit Tests** (by script range):\n`;
              unitTests.forEach(item => comment += `- ${item}\n`);
              comment += `\n`;
            }
            
            if (domainTests.length > 0) {
              comment += `**üèóÔ∏è Domain Tests** (by module):\n`;
              domainTests.forEach(item => comment += `- ${item}\n`);
              comment += `\n`;
            }
            
            if (integrationTests.length > 0) {
              comment += `**üîó Integration Tests** (by suite):\n`;
              integrationTests.forEach(item => comment += `- ${item}\n`);
              comment += `\n`;
            }
            
            // Coverage info if available
            const coverageResult = results.find(r => r.name === 'coverage');
            if (coverageResult && coverageResult.coverage) {
              comment += `### üìà Code Coverage\n\n`;
              comment += `**Coverage:** ${coverageResult.coverage}%\n\n`;
            }
            
            // Next steps for failures
            if (totalFailed > 0) {
              comment += `### ‚ö†Ô∏è Action Required\n\n`;
              comment += `**${totalFailed} test(s) failed.** Please:\n\n`;
              comment += `1. üì• [Download test artifacts](${context.payload.repository.html_url}/actions/runs/${context.runId}) from this workflow run\n`;
              comment += `2. üîç Review failed test details in the artifacts\n`;
              comment += `3. üîß Fix issues and push updates\n`;
              comment += `4. ‚úÖ Re-run tests to verify fixes\n\n`;
            } else {
              comment += `### ‚úÖ All Tests Passed!\n\n`;
              comment += `Great work! All ${totalTests} tests passed successfully.\n\n`;
            }
            
            // Footer
            comment += `---\n`;
            comment += `_Last updated: ${new Date().toISOString()}_  \n`;
            comment += `<!-- WORKFLOW_STATUS_COMMENT:test-execution -->`;
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number
            });
            
            const existingComment = comments.find(c =>
              c.user.login === 'github-actions[bot]' &&
              c.body.includes('WORKFLOW_STATUS_COMMENT:test-execution')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.payload.pull_request.number,
                body: comment
              });
            }

      - name: ‚úÖ Complete
        if: always()
        shell: pwsh
        run: |
          Write-Host "`n‚úÖ Test execution workflow complete!" -ForegroundColor Green
          Write-Host "All test results have been collected and published." -ForegroundColor Cyan

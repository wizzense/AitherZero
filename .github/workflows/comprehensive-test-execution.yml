---
name: üß™ Comprehensive Test Execution

# Run ALL tests (unit + integration) and aggregate results for reporting
on:
  push:
    branches: [main, develop, dev]
    paths:
      - 'domains/**'
      - 'automation-scripts/**'
      - 'tests/**'
      - '**.ps1'
      - '**.psm1'
  pull_request:
    branches: [main, develop, dev]
  schedule:
    # Run daily to catch any regressions
    - cron: '0 2 * * *'  # 2 AM UTC daily
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        type: choice
        options:
          - all
          - unit
          - integration
        default: 'all'

permissions:
  contents: read
  checks: write
  pull-requests: write

concurrency:
  group: comprehensive-tests-${{ github.ref }}
  cancel-in-progress: true

env:
  AITHERZERO_CI: true
  AITHERZERO_NONINTERACTIVE: true

jobs:
  test-discovery:
    name: üìã Discover All Tests
    runs-on: ubuntu-latest
    outputs:
      unit-test-count: ${{ steps.discovery.outputs.unit-count }}
      integration-test-count: ${{ steps.discovery.outputs.integration-count }}
      total-test-count: ${{ steps.discovery.outputs.total-count }}
    
    steps:
      - name: üì• Checkout
        uses: actions/checkout@v4
      
      - name: üîç Discover Test Files
        id: discovery
        shell: pwsh
        run: |
          Write-Host "üîç Discovering all test files..." -ForegroundColor Cyan
          
          $unitTests = @(Get-ChildItem -Path "./tests/unit" -Filter "*.Tests.ps1" -Recurse -ErrorAction SilentlyContinue)
          $integrationTests = @(Get-ChildItem -Path "./tests/integration" -Filter "*.Tests.ps1" -Recurse -ErrorAction SilentlyContinue)
          
          $unitCount = $unitTests.Count
          $integrationCount = $integrationTests.Count
          $totalCount = $unitCount + $integrationCount
          
          Write-Host "üìä Test Discovery Results:" -ForegroundColor Green
          Write-Host "  Unit Tests: $unitCount" -ForegroundColor White
          Write-Host "  Integration Tests: $integrationCount" -ForegroundColor White
          Write-Host "  Total Tests: $totalCount" -ForegroundColor White
          
          # Output for next jobs
          "unit-count=$unitCount" >> $env:GITHUB_OUTPUT
          "integration-count=$integrationCount" >> $env:GITHUB_OUTPUT
          "total-count=$totalCount" >> $env:GITHUB_OUTPUT
          
          # Save test file lists
          $unitTests | ForEach-Object { $_.FullName } | Out-File -FilePath "./unit-tests.txt"
          $integrationTests | ForEach-Object { $_.FullName } | Out-File -FilePath "./integration-tests.txt"
      
      - name: üì§ Upload Test Lists
        uses: actions/upload-artifact@v4
        with:
          name: test-lists
          path: |
            unit-tests.txt
            integration-tests.txt
          retention-days: 7

  run-unit-tests:
    name: üß™ Unit Tests
    runs-on: ubuntu-latest
    needs: test-discovery
    if: |
      (github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'unit' || github.event.inputs.test_type == '') &&
      needs.test-discovery.outputs.unit-test-count > 0
    
    steps:
      - name: üì• Checkout
        uses: actions/checkout@v4
      
      - name: üîß Bootstrap Environment
        shell: pwsh
        run: |
          Write-Host "üöÄ Bootstrapping environment..." -ForegroundColor Cyan
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal
      
      - name: üì¶ Install Testing Tools
        shell: pwsh
        run: |
          Write-Host "üì¶ Installing testing tools..." -ForegroundColor Cyan
          ./automation-scripts/0400_Install-TestingTools.ps1
      
      - name: üß™ Run Unit Tests
        shell: pwsh
        run: |
          Write-Host "üß™ Running ALL unit tests..." -ForegroundColor Cyan
          
          try {
            $result = ./automation-scripts/0402_Run-UnitTests.ps1 `
              -OutputPath "./tests/results" `
              -PassThru `
              -ErrorAction Stop
            
            # Display results
            Write-Host "`nüìä Unit Test Results:" -ForegroundColor Green
            Write-Host "  Total: $($result.TotalCount)" -ForegroundColor White
            Write-Host "  Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "  Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })
            Write-Host "  Skipped: $($result.SkippedCount)" -ForegroundColor Yellow
            
            # Set job summary
            @"
          ## üß™ Unit Test Results
          
          | Metric | Count |
          |--------|-------|
          | Total Tests | $($result.TotalCount) |
          | ‚úÖ Passed | $($result.PassedCount) |
          | ‚ùå Failed | $($result.FailedCount) |
          | ‚è≠Ô∏è Skipped | $($result.SkippedCount) |
          | ‚è±Ô∏è Duration | $($result.Duration.TotalSeconds)s |
          "@ >> $env:GITHUB_STEP_SUMMARY
            
            # Exit with failure if tests failed
            if ($result.FailedCount -gt 0) {
              Write-Host "::error::$($result.FailedCount) unit tests failed"
              exit 1
            }
          }
          catch {
            Write-Host "::error::Unit test execution failed: $_" -ForegroundColor Red
            Write-Host "`nüìä Unit Test Results:" -ForegroundColor Red
            Write-Host "  Total: N/A (execution failed)" -ForegroundColor Red
            Write-Host "  Passed: N/A" -ForegroundColor Red
            Write-Host "  Failed: N/A" -ForegroundColor Red
            Write-Host "  Skipped: N/A" -ForegroundColor Red
            
            @"
          ## üß™ Unit Test Results
          
          ‚ùå **Test execution failed**
          
          Error: $_
          
          Please check the logs for details.
          "@ >> $env:GITHUB_STEP_SUMMARY
            
            exit 1
          }
      
      - name: üì§ Upload Unit Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results
          path: |
            tests/results/*.xml
            tests/results/*.json
          retention-days: 30

  run-integration-tests:
    name: üîó Integration Tests
    runs-on: ubuntu-latest
    needs: test-discovery
    if: |
      (github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'integration' || github.event.inputs.test_type == '') &&
      needs.test-discovery.outputs.integration-test-count > 0
    
    steps:
      - name: üì• Checkout
        uses: actions/checkout@v4
      
      - name: üîß Bootstrap Environment
        shell: pwsh
        run: |
          Write-Host "üöÄ Bootstrapping environment..." -ForegroundColor Cyan
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal
      
      - name: üì¶ Install Testing Tools
        shell: pwsh
        run: |
          Write-Host "üì¶ Installing testing tools..." -ForegroundColor Cyan
          ./automation-scripts/0400_Install-TestingTools.ps1
      
      - name: üîó Run Integration Tests
        shell: pwsh
        run: |
          Write-Host "üîó Running ALL integration tests..." -ForegroundColor Cyan
          
          try {
            $result = ./automation-scripts/0403_Run-IntegrationTests.ps1 `
              -OutputPath "./tests/results" `
              -PassThru `
              -ErrorAction Stop
            
            # Display results
            Write-Host "`nüìä Integration Test Results:" -ForegroundColor Green
            Write-Host "  Total: $($result.TotalCount)" -ForegroundColor White
            Write-Host "  Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "  Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })
            Write-Host "  Skipped: $($result.SkippedCount)" -ForegroundColor Yellow
            
            # Set job summary
            @"
          ## üîó Integration Test Results
          
          | Metric | Count |
          |--------|-------|
          | Total Tests | $($result.TotalCount) |
          | ‚úÖ Passed | $($result.PassedCount) |
          | ‚ùå Failed | $($result.FailedCount) |
          | ‚è≠Ô∏è Skipped | $($result.SkippedCount) |
          | ‚è±Ô∏è Duration | $($result.Duration.TotalSeconds)s |
          "@ >> $env:GITHUB_STEP_SUMMARY
            
            # Exit with failure if tests failed
            if ($result.FailedCount -gt 0) {
              Write-Host "::error::$($result.FailedCount) integration tests failed"
              exit 1
            }
          }
          catch {
            Write-Host "::error::Integration test execution failed: $_" -ForegroundColor Red
            Write-Host "`nüìä Integration Test Results:" -ForegroundColor Red
            Write-Host "  Total: N/A (execution failed)" -ForegroundColor Red
            Write-Host "  Passed: N/A" -ForegroundColor Red
            Write-Host "  Failed: N/A" -ForegroundColor Red
            Write-Host "  Skipped: N/A" -ForegroundColor Red
            
            @"
          ## üîó Integration Test Results
          
          ‚ùå **Test execution failed**
          
          Error: $_
          
          Please check the logs for details.
          "@ >> $env:GITHUB_STEP_SUMMARY
            
            exit 1
          }
      
      - name: üì§ Upload Integration Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: |
            tests/results/*.xml
            tests/results/*.json
          retention-days: 30

  aggregate-results:
    name: üìä Aggregate All Test Results
    runs-on: ubuntu-latest
    needs: [test-discovery, run-unit-tests, run-integration-tests]
    if: always()
    
    steps:
      - name: üì• Checkout
        uses: actions/checkout@v4
      
      - name: üì• Download All Test Results
        uses: actions/download-artifact@v4
        with:
          path: ./test-artifacts
      
      - name: üìä Aggregate Results
        shell: pwsh
        run: |
          Write-Host "üìä Aggregating all test results..." -ForegroundColor Cyan
          
          # Parse test discovery outputs with proper error handling
          $discoveryTotal = '${{ needs.test-discovery.outputs.total-test-count }}'
          $discoveryUnit = '${{ needs.test-discovery.outputs.unit-test-count }}'
          $discoveryIntegration = '${{ needs.test-discovery.outputs.integration-test-count }}'
          
          # Helper function to convert string to int or zero
          function ConvertToIntOrZero($value) {
            if ($value -match '^\d+$') { return [int]$value } else { return 0 }
          }
          
          # Convert to integers with fallback to 0
          $totalTestFiles = ConvertToIntOrZero $discoveryTotal
          $unitTestFiles = ConvertToIntOrZero $discoveryUnit
          $integrationTestFiles = ConvertToIntOrZero $discoveryIntegration
          
          Write-Host "Test discovery outputs:" -ForegroundColor Yellow
          Write-Host "  Total: $totalTestFiles" -ForegroundColor White
          Write-Host "  Unit: $unitTestFiles" -ForegroundColor White
          Write-Host "  Integration: $integrationTestFiles" -ForegroundColor White
          
          # Create aggregated report structure
          $aggregatedReport = @{
            Timestamp = (Get-Date).ToString('o')
            TotalTestFiles = $totalTestFiles
            UnitTestFiles = $unitTestFiles
            IntegrationTestFiles = $integrationTestFiles
            TestResults = @{
              Summary = @{
                Total = 0
                Passed = 0
                Failed = 0
                Skipped = 0
              }
              ByType = @{
                Unit = @{ Total = 0; Passed = 0; Failed = 0; Skipped = 0 }
                Integration = @{ Total = 0; Passed = 0; Failed = 0; Skipped = 0 }
                Unknown = @{ Total = 0; Passed = 0; Failed = 0; Skipped = 0 }
              }
              Details = @()
            }
            Workflow = @{
              RunId = '${{ github.run_id }}'
              RunNumber = '${{ github.run_number }}'
              Ref = '${{ github.ref }}'
              Sha = '${{ github.sha }}'
              Actor = '${{ github.actor }}'
              EventName = '${{ github.event_name }}'
            }
          }
          
          # Track unique test files processed
          $processedUnitFiles = @{}
          $processedIntegrationFiles = @{}
          
          # Parse all test result files
          $resultFiles = Get-ChildItem -Path "./test-artifacts" -Filter "*.json" -Recurse -ErrorAction SilentlyContinue
          
          Write-Host "Found $($resultFiles.Count) result files" -ForegroundColor Yellow
          
          foreach ($file in $resultFiles) {
            try {
              $result = Get-Content $file.FullName -Raw | ConvertFrom-Json
              
              if ($null -ne $result.TotalCount) {
                # Determine test type by checking parent directory name or file name
                if ($file.FullName -match 'unit-test-results|TestReport-Unit') {
                  $testType = 'Unit'
                } elseif ($file.FullName -match 'integration-test-results|TestReport-Integration') {
                  $testType = 'Integration'
                } else {
                  $testType = 'Unknown'
                  Write-Warning "Could not determine test type for: $($file.FullName). Defaulting to 'Unknown'."
                }
                
                # Track unique test files if available
                if ($result.TestFiles) {
                  foreach ($testFile in $result.TestFiles) {
                    if ($testType -eq 'Unit') {
                      $processedUnitFiles[$testFile] = $true
                    } elseif ($testType -eq 'Integration') {
                      $processedIntegrationFiles[$testFile] = $true
                    }
                  }
                }
                
                # Add to summary
                $aggregatedReport.TestResults.Summary.Total += $result.TotalCount
                $aggregatedReport.TestResults.Summary.Passed += $result.PassedCount
                $aggregatedReport.TestResults.Summary.Failed += $result.FailedCount
                $aggregatedReport.TestResults.Summary.Skipped += $result.SkippedCount
                
                # Add to by-type
                $aggregatedReport.TestResults.ByType.$testType.Total += $result.TotalCount
                $aggregatedReport.TestResults.ByType.$testType.Passed += $result.PassedCount
                $aggregatedReport.TestResults.ByType.$testType.Failed += $result.FailedCount
                $aggregatedReport.TestResults.ByType.$testType.Skipped += $result.SkippedCount
                
                Write-Host "  Processed: $($file.Name) - $testType" -ForegroundColor Green
              }
            }
            catch {
              Write-Warning "Failed to parse: $($file.Name)"
            }
          }
          
          # Fallback: If discovery didn't capture test files, count from processed results
          if ($aggregatedReport.TotalTestFiles -eq 0 -and ($processedUnitFiles.Count -gt 0 -or $processedIntegrationFiles.Count -gt 0)) {
            $aggregatedReport.UnitTestFiles = $processedUnitFiles.Count
            $aggregatedReport.IntegrationTestFiles = $processedIntegrationFiles.Count
            $aggregatedReport.TotalTestFiles = $processedUnitFiles.Count + $processedIntegrationFiles.Count
            Write-Host "Using fallback test file counts from processed results" -ForegroundColor Yellow
          }
          
          # Additional fallback: Count test files directly if still 0
          if ($aggregatedReport.TotalTestFiles -eq 0) {
            Write-Host "Discovery and result parsing failed to find test files, counting directly..." -ForegroundColor Yellow
            $unitTests = @(Get-ChildItem -Path "./tests/unit" -Filter "*.Tests.ps1" -Recurse -ErrorAction SilentlyContinue)
            $integrationTests = @(Get-ChildItem -Path "./tests/integration" -Filter "*.Tests.ps1" -Recurse -ErrorAction SilentlyContinue)
            $aggregatedReport.UnitTestFiles = $unitTests.Count
            $aggregatedReport.IntegrationTestFiles = $integrationTests.Count
            $aggregatedReport.TotalTestFiles = $unitTests.Count + $integrationTests.Count
            Write-Host "Direct count: Unit=$($unitTests.Count), Integration=$($integrationTests.Count)" -ForegroundColor Cyan
          }
          
          # Save aggregated report
          $reportPath = "./reports/TestReport-Aggregated-$(Get-Date -Format 'yyyyMMdd-HHmmss').json"
          New-Item -ItemType Directory -Path "./reports" -Force | Out-Null
          $aggregatedReport | ConvertTo-Json -Depth 10 | Set-Content -Path $reportPath
          
          Write-Host "`n‚úÖ Aggregated Report Saved: $reportPath" -ForegroundColor Green
          Write-Host "`nüìä Final Results:" -ForegroundColor Cyan
          Write-Host "  Test Files Discovered: $($aggregatedReport.TotalTestFiles)" -ForegroundColor White
          Write-Host "  Tests Executed: $($aggregatedReport.TestResults.Summary.Total)" -ForegroundColor White
          Write-Host "  ‚úÖ Passed: $($aggregatedReport.TestResults.Summary.Passed)" -ForegroundColor Green
          Write-Host "  ‚ùå Failed: $($aggregatedReport.TestResults.Summary.Failed)" -ForegroundColor $(if ($aggregatedReport.TestResults.Summary.Failed -gt 0) { 'Red' } else { 'Green' })
          Write-Host "  ‚è≠Ô∏è Skipped: $($aggregatedReport.TestResults.Summary.Skipped)" -ForegroundColor Yellow
          
          # Create job summary
          @"
          # üß™ Comprehensive Test Execution Summary
          
          ## üìä Overall Results
          
          | Metric | Count |
          |--------|-------|
          | Test Files Discovered | $($aggregatedReport.TotalTestFiles) |
          | Tests Executed | $($aggregatedReport.TestResults.Summary.Total) |
          | ‚úÖ Passed | $($aggregatedReport.TestResults.Summary.Passed) |
          | ‚ùå Failed | $($aggregatedReport.TestResults.Summary.Failed) |
          | ‚è≠Ô∏è Skipped | $($aggregatedReport.TestResults.Summary.Skipped) |
          
          ## üìã By Test Type
          
          ### Unit Tests
          | Metric | Count |
          |--------|-------|
          | Total | $($aggregatedReport.TestResults.ByType.Unit.Total) |
          | ‚úÖ Passed | $($aggregatedReport.TestResults.ByType.Unit.Passed) |
          | ‚ùå Failed | $($aggregatedReport.TestResults.ByType.Unit.Failed) |
          
          ### Integration Tests
          | Metric | Count |
          |--------|-------|
          | Total | $($aggregatedReport.TestResults.ByType.Integration.Total) |
          | ‚úÖ Passed | $($aggregatedReport.TestResults.ByType.Integration.Passed) |
          | ‚ùå Failed | $($aggregatedReport.TestResults.ByType.Integration.Failed) |
          "@ >> $env:GITHUB_STEP_SUMMARY
      
      - name: üì§ Upload Aggregated Report
        uses: actions/upload-artifact@v4
        with:
          name: aggregated-test-report
          path: reports/
          retention-days: 90
      
      - name: üí¨ Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              const reportFile = fs.readdirSync('./reports').find(f => f.startsWith('TestReport-Aggregated'));
              
              if (!reportFile) {
                throw new Error('No aggregated test report found');
              }
              
              const report = JSON.parse(fs.readFileSync(`./reports/${reportFile}`, 'utf8'));
              
              // Safely calculate pass rate with proper null/zero checks
              const totalTests = report.TestResults?.Summary?.Total || 0;
              const passedTests = report.TestResults?.Summary?.Passed || 0;
              const failedTests = report.TestResults?.Summary?.Failed || 0;
              const skippedTests = report.TestResults?.Summary?.Skipped || 0;
              
              const passRate = totalTests > 0 
                ? ((passedTests / totalTests) * 100).toFixed(1) 
                : '0.0';
              
              const statusIcon = totalTests === 0 ? '‚ö†Ô∏è' : (failedTests === 0 ? '‚úÖ' : '‚ùå');
              
              // Get test file counts with fallback values
              const totalTestFiles = report.TotalTestFiles || 0;
              const unitTestFiles = report.UnitTestFiles || 0;
              const integrationTestFiles = report.IntegrationTestFiles || 0;
              
              // Get by-type results with safe fallbacks
              const unitTotal = report.TestResults?.ByType?.Unit?.Total || 0;
              const unitPassed = report.TestResults?.ByType?.Unit?.Passed || 0;
              const unitFailed = report.TestResults?.ByType?.Unit?.Failed || 0;
              const unitSkipped = report.TestResults?.ByType?.Unit?.Skipped || 0;
              
              const integrationTotal = report.TestResults?.ByType?.Integration?.Total || 0;
              const integrationPassed = report.TestResults?.ByType?.Integration?.Passed || 0;
              const integrationFailed = report.TestResults?.ByType?.Integration?.Failed || 0;
              const integrationSkipped = report.TestResults?.ByType?.Integration?.Skipped || 0;
              
              // Build test file discovery message
              const discoveryMessage = totalTestFiles > 0
                ? `${totalTestFiles} test files found (${unitTestFiles} unit, ${integrationTestFiles} integration)`
                : `Test files: ${unitTestFiles} unit, ${integrationTestFiles} integration`;
              
              const comment = `## ${statusIcon} Comprehensive Test Results
              
              **Test Discovery:** ${discoveryMessage}
              **Tests Executed:** ${totalTests}
              **Pass Rate:** ${passRate}%
              
              | Type | Total | ‚úÖ Passed | ‚ùå Failed | ‚è≠Ô∏è Skipped |
              |------|-------|----------|----------|-----------|
              | **Unit** | ${unitTotal} | ${unitPassed} | ${unitFailed} | ${unitSkipped} |
              | **Integration** | ${integrationTotal} | ${integrationPassed} | ${integrationFailed} | ${integrationSkipped} |
              | **Total** | ${totalTests} | ${passedTests} | ${failedTests} | ${skippedTests} |
              
              ---
              üìä [View Full Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
              `;
              
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
              
            } catch (error) {
              console.error('Failed to create PR comment:', error);
              
              // Create a fallback comment with basic info
              const fallbackComment = `## ‚ö†Ô∏è Test Results
              
              Test execution completed, but the aggregated report could not be loaded.
              
              Please check the [workflow run logs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}) for detailed results.
              
              Error: ${error.message}
              `;
              
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: fallbackComment
              });
            }

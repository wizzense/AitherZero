---
name: âš¡ Parallel Testing (High Performance)

# High-performance parallel test execution across multiple runners
# Tests are split by range and executed concurrently, then consolidated

on:
  push:
    branches: [main, develop, dev, dev-staging, ring-0-integrations]
    paths:
      - 'aithercore/**'
      - 'library/automation-scripts/**'
      - 'tests/**'
      - '**.ps1'
      - '**.psm1'
  pull_request:
    branches: [main, develop, dev, dev-staging, ring-0-integrations]
  workflow_dispatch:
    inputs:
      test_filter:
        description: 'Test filter (all, unit, integration, domains)'
        type: choice
        options:
          - all
          - unit
          - integration
          - domains
        default: 'all'

permissions:
  contents: read
  checks: write
  pull-requests: write

# Allow parallel execution across different test suites
# Cancel old runs for the same branch when new commits arrive
concurrency:
  group: parallel-tests-${{ github.ref }}
  cancel-in-progress: true

env:
  AITHERZERO_CI: true
  AITHERZERO_NONINTERACTIVE: true

jobs:
  # Fast preparation job - sets up for parallel execution
  prepare:
    name: ğŸ¯ Prepare Test Matrix
    # Skip if triggered by bot to prevent unnecessary runs
    if: github.actor != 'github-actions[bot]'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      unit-ranges: ${{ steps.matrix.outputs.unit-ranges }}
      domain-modules: ${{ steps.matrix.outputs.domain-modules }}
      integration-suites: ${{ steps.matrix.outputs.integration-suites }}

    steps:
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4

      - name: ğŸ”§ Quick Bootstrap
        shell: pwsh
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: ğŸ¯ Generate Test Matrix
        id: matrix
        shell: pwsh
        run: |
          # Unit test ranges (by script number ranges)
          $unitRanges = @(
            '0000-0099',
            '0100-0199',
            '0200-0299',
            '0400-0499',
            '0500-0599',
            '0700-0799',
            '0800-0899',
            '0900-0999'
          )

          # Domain modules - ONLY modules that have test directories
          $domainModules = @(
            'configuration',
            'documentation',
            'infrastructure',
            'security',
            'testing',
            'utilities'
          )

          # Integration test suites - ONLY suites that exist
          $integrationSuites = @(
            'automation-scripts'
          )

          # Output as JSON arrays for matrix
          $unitRangesJson = $unitRanges | ConvertTo-Json -Compress
          $domainModulesJson = $domainModules | ConvertTo-Json -Compress
          $integrationSuitesJson = $integrationSuites | ConvertTo-Json -Compress

          "unit-ranges=$unitRangesJson" >> $env:GITHUB_OUTPUT
          "domain-modules=$domainModulesJson" >> $env:GITHUB_OUTPUT
          "integration-suites=$integrationSuitesJson" >> $env:GITHUB_OUTPUT

          Write-Host "âœ… Test matrix generated" -ForegroundColor Green
          Write-Host "   Unit ranges: $($unitRanges.Count)" -ForegroundColor Cyan
          Write-Host "   Domain modules: $($domainModules.Count)" -ForegroundColor Cyan
          Write-Host "   Integration suites: $($integrationSuites.Count)" -ForegroundColor Cyan

  # Parallel unit tests by script number range
  unit-tests-parallel:
    name: ğŸ§ª Unit Tests [${{ matrix.range }}]
    needs: prepare
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: |
      github.actor != 'github-actions[bot]' && 
      (github.event.inputs.test_filter == 'all' || github.event.inputs.test_filter == 'unit' || github.event.inputs.test_filter == '')

    strategy:
      fail-fast: false  # Continue all jobs even if one fails
      max-parallel: 8   # Run up to 8 jobs simultaneously
      matrix:
        range: ${{ fromJson(needs.prepare.outputs.unit-ranges) }}

    steps:
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4

      - name: ğŸ”§ Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: ğŸ§ª Run Unit Tests [${{ matrix.range }}]
        id: run-tests
        shell: pwsh
        continue-on-error: true
        timeout-minutes: 5
        run: |
          Write-Host "âš¡ Running unit tests for range: ${{ matrix.range }}" -ForegroundColor Cyan

          $testPath = "./tests/unit/library/automation-scripts/${{ matrix.range }}"

          if (Test-Path $testPath) {
            $config = New-PesterConfiguration
            $config.Run.Path = $testPath
            $config.Run.Exit = $false
            $config.Run.PassThru = $true
            $config.Output.Verbosity = 'Normal'
            $config.TestResult.Enabled = $true
            $config.TestResult.OutputPath = "./tests/results/UnitTests-${{ matrix.range }}.xml"
            $config.TestResult.OutputFormat = 'NUnitXml'

            $result = Invoke-Pester -Configuration $config

            Write-Host "âœ… Tests completed for ${{ matrix.range }}" -ForegroundColor Green
            Write-Host "   Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "   Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })
            Write-Host "   Skipped: $($result.SkippedCount)" -ForegroundColor Yellow

            # Set outputs for status check
            "passed=$($result.PassedCount)" >> $env:GITHUB_OUTPUT
            "failed=$($result.FailedCount)" >> $env:GITHUB_OUTPUT
            "skipped=$($result.SkippedCount)" >> $env:GITHUB_OUTPUT

            # Mark step as failed but don't exit (continue-on-error allows continuation)
            if ($result.FailedCount -gt 0) {
              Write-Host "##vso[task.complete result=Failed;]Tests failed"
              exit 1
            }
          } else {
            Write-Host "â­ï¸  No tests found for range ${{ matrix.range }}" -ForegroundColor Yellow
            "passed=0" >> $env:GITHUB_OUTPUT
            "failed=0" >> $env:GITHUB_OUTPUT
            "skipped=0" >> $env:GITHUB_OUTPUT
          }

      - name: ğŸ“‹ Create Status Check
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        env:
          PASSED: ${{ steps.run-tests.outputs.passed }}
          FAILED: ${{ steps.run-tests.outputs.failed }}
          SKIPPED: ${{ steps.run-tests.outputs.skipped }}
          RANGE: ${{ matrix.range }}
        with:
          script: |
            const passed = parseInt(process.env.PASSED || '0', 10);
            const failed = parseInt(process.env.FAILED || '0', 10);
            const skipped = parseInt(process.env.SKIPPED || '0', 10);
            const range = process.env.RANGE;
            const total = passed + failed + skipped;

            let conclusion, summary;

            if (total === 0) {
              conclusion = 'success';
              summary = `â­ï¸ No tests in range ${range}`;
            } else if (failed === 0) {
              conclusion = 'success';
              summary = `âœ… ${passed} test(s) passed in range ${range}`;
            } else {
              conclusion = 'failure';
              summary = `âŒ ${failed} test(s) failed in range ${range}`;
            }

            await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: `ğŸ§ª Unit Tests [${range}]`,
              head_sha: context.payload.pull_request.head.sha,
              status: 'completed',
              conclusion: conclusion,
              output: {
                title: summary,
                summary: summary,
                text: `**Range:** ${range}\n**Passed:** ${passed}\n**Failed:** ${failed}\n**Skipped:** ${skipped}\n**Total:** ${total}`
              }
            });

      - name: ğŸ“Š Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-tests-${{ matrix.range }}
          path: ./tests/results/UnitTests-${{ matrix.range }}.xml
          retention-days: 7
          if-no-files-found: warn

      - name: âŒ Fail Job if Tests Failed
        if: steps.run-tests.outputs.failed != '0' && steps.run-tests.outputs.failed != ''
        shell: pwsh
        run: |
          $failed = "${{ steps.run-tests.outputs.failed }}"
          Write-Host "âŒ Job must fail: $failed test(s) failed in range ${{ matrix.range }}" -ForegroundColor Red
          exit 1

  # Parallel domain module tests
  domain-tests-parallel:
    name: ğŸ—ï¸ Domain Tests [${{ matrix.module }}]
    needs: prepare
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: github.event.inputs.test_filter == 'all' || github.event.inputs.test_filter == 'domains' || github.event.inputs.test_filter == ''

    strategy:
      fail-fast: false
      max-parallel: 8
      matrix:
        module: ${{ fromJson(needs.prepare.outputs.domain-modules) }}

    steps:
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4

      - name: ğŸ”§ Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: ğŸ—ï¸ Run Domain Tests [${{ matrix.module }}]
        id: run-tests
        shell: pwsh
        continue-on-error: true
        timeout-minutes: 5
        run: |
          Write-Host "âš¡ Running domain tests for: ${{ matrix.module }}" -ForegroundColor Cyan

          $testPath = "./tests/aithercore/${{ matrix.module }}"

          if (Test-Path $testPath) {
            $config = New-PesterConfiguration
            $config.Run.Path = $testPath
            $config.Run.Exit = $false
            $config.Run.PassThru = $true
            $config.Output.Verbosity = 'Normal'
            $config.TestResult.Enabled = $true
            $config.TestResult.OutputPath = "./tests/results/DomainTests-${{ matrix.module }}.xml"
            $config.TestResult.OutputFormat = 'NUnitXml'

            $result = Invoke-Pester -Configuration $config

            Write-Host "âœ… Tests completed for ${{ matrix.module }}" -ForegroundColor Green
            Write-Host "   Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "   Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })
            Write-Host "   Skipped: $($result.SkippedCount)" -ForegroundColor Yellow

            # Set outputs for status check
            "passed=$($result.PassedCount)" >> $env:GITHUB_OUTPUT
            "failed=$($result.FailedCount)" >> $env:GITHUB_OUTPUT
            "skipped=$($result.SkippedCount)" >> $env:GITHUB_OUTPUT

            # Mark step as failed but don't exit (continue-on-error allows continuation)
            if ($result.FailedCount -gt 0) {
              Write-Host "##vso[task.complete result=Failed;]Tests failed"
              exit 1
            }
          } else {
            Write-Host "â­ï¸  No tests found for module ${{ matrix.module }}" -ForegroundColor Yellow
            "passed=0" >> $env:GITHUB_OUTPUT
            "failed=0" >> $env:GITHUB_OUTPUT
            "skipped=0" >> $env:GITHUB_OUTPUT
          }

      - name: ğŸ“‹ Create Status Check
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        env:
          PASSED: ${{ steps.run-tests.outputs.passed }}
          FAILED: ${{ steps.run-tests.outputs.failed }}
          SKIPPED: ${{ steps.run-tests.outputs.skipped }}
          MODULE: ${{ matrix.module }}
        with:
          script: |
            const passed = parseInt(process.env.PASSED || '0', 10);
            const failed = parseInt(process.env.FAILED || '0', 10);
            const skipped = parseInt(process.env.SKIPPED || '0', 10);
            const module = process.env.MODULE;
            const total = passed + failed + skipped;

            let conclusion, summary;

            if (total === 0) {
              conclusion = 'success';
              summary = `â­ï¸ No tests for ${module} domain`;
            } else if (failed === 0) {
              conclusion = 'success';
              summary = `âœ… ${passed} test(s) passed for ${module} domain`;
            } else {
              conclusion = 'failure';
              summary = `âŒ ${failed} test(s) failed for ${module} domain`;
            }

            await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: `ğŸ—ï¸ Domain Tests [${module}]`,
              head_sha: context.payload.pull_request.head.sha,
              status: 'completed',
              conclusion: conclusion,
              output: {
                title: summary,
                summary: summary,
                text: `**Domain:** ${module}\n**Passed:** ${passed}\n**Failed:** ${failed}\n**Skipped:** ${skipped}\n**Total:** ${total}`
              }
            });

      - name: ğŸ“Š Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: domain-tests-${{ matrix.module }}
          path: ./tests/results/DomainTests-${{ matrix.module }}.xml
          retention-days: 7
          if-no-files-found: warn

      - name: âŒ Fail Job if Tests Failed
        if: steps.run-tests.outputs.failed != '0' && steps.run-tests.outputs.failed != ''
        shell: pwsh
        run: |
          $failed = "${{ steps.run-tests.outputs.failed }}"
          Write-Host "âŒ Job must fail: $failed test(s) failed in ${{ matrix.module }} domain" -ForegroundColor Red
          exit 1

  # Parallel integration tests
  integration-tests-parallel:
    name: ğŸ”— Integration Tests [${{ matrix.suite }}]
    needs: prepare
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event.inputs.test_filter == 'all' || github.event.inputs.test_filter == 'integration' || github.event.inputs.test_filter == ''

    strategy:
      fail-fast: false
      max-parallel: 3
      matrix:
        suite: ${{ fromJson(needs.prepare.outputs.integration-suites) }}

    steps:
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4

      - name: ğŸ”§ Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: ğŸ”— Run Integration Tests [${{ matrix.suite }}]
        id: run-tests
        shell: pwsh
        continue-on-error: true
        timeout-minutes: 10
        run: |
          Write-Host "âš¡ Running integration tests for: ${{ matrix.suite }}" -ForegroundColor Cyan

          $testPath = "./tests/integration/${{ matrix.suite }}"

          if (Test-Path $testPath) {
            $config = New-PesterConfiguration
            $config.Run.Path = $testPath
            $config.Run.Exit = $false
            $config.Run.PassThru = $true
            $config.Output.Verbosity = 'Normal'
            $config.TestResult.Enabled = $true
            $config.TestResult.OutputPath = "./tests/results/IntegrationTests-${{ matrix.suite }}.xml"
            $config.TestResult.OutputFormat = 'NUnitXml'

            $result = Invoke-Pester -Configuration $config

            Write-Host "âœ… Tests completed for ${{ matrix.suite }}" -ForegroundColor Green
            Write-Host "   Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "   Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })
            Write-Host "   Skipped: $($result.SkippedCount)" -ForegroundColor Yellow

            # Set outputs for status check
            "passed=$($result.PassedCount)" >> $env:GITHUB_OUTPUT
            "failed=$($result.FailedCount)" >> $env:GITHUB_OUTPUT
            "skipped=$($result.SkippedCount)" >> $env:GITHUB_OUTPUT

            # Mark step as failed but don't exit (continue-on-error allows continuation)
            if ($result.FailedCount -gt 0) {
              Write-Host "##vso[task.complete result=Failed;]Tests failed"
              exit 1
            }
          } else {
            Write-Host "â­ï¸  No tests found for suite ${{ matrix.suite }}" -ForegroundColor Yellow
            "passed=0" >> $env:GITHUB_OUTPUT
            "failed=0" >> $env:GITHUB_OUTPUT
            "skipped=0" >> $env:GITHUB_OUTPUT
          }

      - name: ğŸ“‹ Create Status Check
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        env:
          PASSED: ${{ steps.run-tests.outputs.passed }}
          FAILED: ${{ steps.run-tests.outputs.failed }}
          SKIPPED: ${{ steps.run-tests.outputs.skipped }}
          SUITE: ${{ matrix.suite }}
        with:
          script: |
            const passed = parseInt(process.env.PASSED || '0', 10);
            const failed = parseInt(process.env.FAILED || '0', 10);
            const skipped = parseInt(process.env.SKIPPED || '0', 10);
            const suite = process.env.SUITE;
            const total = passed + failed + skipped;

            let conclusion, summary;

            if (total === 0) {
              conclusion = 'success';
              summary = `â­ï¸ No tests for ${suite} suite`;
            } else if (failed === 0) {
              conclusion = 'success';
              summary = `âœ… ${passed} test(s) passed for ${suite} suite`;
            } else {
              conclusion = 'failure';
              summary = `âŒ ${failed} test(s) failed for ${suite} suite`;
            }

            await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: `ğŸ”— Integration Tests [${suite}]`,
              head_sha: context.payload.pull_request.head.sha,
              status: 'completed',
              conclusion: conclusion,
              output: {
                title: summary,
                summary: summary,
                text: `**Suite:** ${suite}\n**Passed:** ${passed}\n**Failed:** ${failed}\n**Skipped:** ${skipped}\n**Total:** ${total}`
              }
            });

      - name: ğŸ“Š Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-tests-${{ matrix.suite }}
          path: ./tests/results/IntegrationTests-${{ matrix.suite }}.xml
          retention-days: 7
          if-no-files-found: warn

      - name: âŒ Fail Job if Tests Failed
        if: steps.run-tests.outputs.failed != '0' && steps.run-tests.outputs.failed != ''
        shell: pwsh
        run: |
          $failed = "${{ steps.run-tests.outputs.failed }}"
          Write-Host "âŒ Job must fail: $failed test(s) failed in ${{ matrix.suite }} suite" -ForegroundColor Red
          exit 1

  # Static analysis (runs in parallel with tests)
  static-analysis:
    name: ğŸ” Static Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: github.event.inputs.test_filter == 'all' || github.event.inputs.test_filter == ''

    steps:
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4

      - name: ğŸ”§ Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: ğŸ” Syntax Validation
        shell: pwsh
        timeout-minutes: 2
        run: |
          Write-Host "âš¡ Running syntax validation" -ForegroundColor Cyan
          ./library/automation-scripts/0407_Validate-Syntax.ps1 -All

      - name: ğŸ” PSScriptAnalyzer
        id: pssa
        shell: pwsh
        timeout-minutes: 5
        continue-on-error: true
        run: |
          Write-Host "âš¡ Running PSScriptAnalyzer" -ForegroundColor Cyan
          ./library/automation-scripts/0404_Run-PSScriptAnalyzer.ps1

          # Capture summary for later use
          $summaryPath = Get-ChildItem "./tests/analysis" -Filter "PSScriptAnalyzer-Summary-*.json" -ErrorAction SilentlyContinue |
            Sort-Object LastWriteTime -Descending | Select-Object -First 1

          if ($summaryPath) {
            $summary = Get-Content $summaryPath.FullName | ConvertFrom-Json
            "error-count=$($summary.BySeverity.Error)" >> $env:GITHUB_OUTPUT
            "warning-count=$($summary.BySeverity.Warning)" >> $env:GITHUB_OUTPUT
            "info-count=$($summary.BySeverity.Information)" >> $env:GITHUB_OUTPUT
            "total-issues=$($summary.TotalIssues)" >> $env:GITHUB_OUTPUT
          }

      - name: ğŸ“Š Code Coverage Analysis
        id: coverage
        shell: pwsh
        timeout-minutes: 3
        continue-on-error: true
        run: |
          Write-Host "âš¡ Analyzing code coverage from test runs..." -ForegroundColor Cyan

          # Check for coverage files from test runs
          $coverageFiles = Get-ChildItem "./tests/coverage" -Filter "Coverage-*.xml" -ErrorAction SilentlyContinue

          if ($coverageFiles) {
            Write-Host "Found $($coverageFiles.Count) coverage file(s)" -ForegroundColor Cyan

            # Parse latest coverage file
            $latestCoverage = $coverageFiles | Sort-Object LastWriteTime -Descending | Select-Object -First 1
            [xml]$covXml = Get-Content $latestCoverage.FullName

            $lineRate = [math]::Round([double]$covXml.coverage.'line-rate' * 100, 2)
            $branchRate = [math]::Round([double]$covXml.coverage.'branch-rate' * 100, 2)

            "line-coverage=$lineRate" >> $env:GITHUB_OUTPUT
            "branch-coverage=$branchRate" >> $env:GITHUB_OUTPUT

            Write-Host "âœ… Line Coverage: $lineRate%" -ForegroundColor Green
            Write-Host "âœ… Branch Coverage: $branchRate%" -ForegroundColor Green
          } else {
            Write-Host "âš ï¸  No coverage files found" -ForegroundColor Yellow
            "line-coverage=0" >> $env:GITHUB_OUTPUT
            "branch-coverage=0" >> $env:GITHUB_OUTPUT
          }

      - name: ğŸ“Š Upload Analysis Results
        if: always() && hashFiles('./tests/analysis/*') != ''
        uses: actions/upload-artifact@v4
        with:
          name: static-analysis
          path: ./tests/analysis/*
          retention-days: 7

      - name: ğŸ› Collect Open Issues
        id: issues
        if: always()
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const { data: issues } = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              per_page: 100
            });

            const bugs = issues.filter(i => i.labels.some(l => l.name === 'bug'));
            const features = issues.filter(i => i.labels.some(l => l.name === 'enhancement' || l.name === 'feature'));
            const critical = issues.filter(i => i.labels.some(l => l.name === 'critical' || l.name === 'priority: high'));

            core.setOutput('total-issues', issues.length);
            core.setOutput('bugs', bugs.length);
            core.setOutput('features', features.length);
            core.setOutput('critical', critical.length);

            // Save detailed issue data
            const fs = require('fs');
            const issuesDir = './reports';
            if (!fs.existsSync(issuesDir)) {
              fs.mkdirSync(issuesDir, { recursive: true });
            }

            const issueData = {
              timestamp: new Date().toISOString(),
              total: issues.length,
              bugs: bugs.length,
              features: features.length,
              critical: critical.length,
              issues: issues.map(i => ({
                number: i.number,
                title: i.title,
                state: i.state,
                labels: i.labels.map(l => l.name),
                created_at: i.created_at,
                updated_at: i.updated_at,
                url: i.html_url
              }))
            };

            fs.writeFileSync('./reports/open-issues.json', JSON.stringify(issueData, null, 2));
            console.log(`âœ… Collected ${issues.length} open issues`);

      - name: â±ï¸ Performance Metrics
        id: performance
        if: always()
        shell: pwsh
        continue-on-error: true
        run: |
          Write-Host "âš¡ Collecting performance metrics..." -ForegroundColor Cyan

          # Calculate workflow execution time
          $startTime = "${{ github.event.workflow_run.created_at || github.event.head_commit.timestamp }}"
          if ($startTime) {
            $start = [DateTime]::Parse($startTime)
            $duration = (Get-Date) - $start
            $durationMinutes = [math]::Round($duration.TotalMinutes, 2)
            "workflow-duration=$durationMinutes" >> $env:GITHUB_OUTPUT
            Write-Host "Workflow duration: $durationMinutes minutes" -ForegroundColor Cyan
          }

          # Collect performance data from logs if available
          $perfData = @{
            Timestamp = (Get-Date -Format "yyyy-MM-ddTHH:mm:ss.fffZ")
            WorkflowRun = "${{ github.run_id }}"
            Metrics = @{
              TestExecutionTime = $durationMinutes
              ParallelJobs = 17  # Total jobs in parallel execution
              AverageJobDuration = 0
            }
          }

          $perfData | ConvertTo-Json -Depth 10 | Out-File -FilePath "./reports/performance-metrics.json" -Encoding UTF8
          Write-Host "âœ… Performance metrics collected" -ForegroundColor Green

      - name: ğŸ“‹ Create Status Check
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        env:
          ERRORS: ${{ steps.pssa.outputs.error-count }}
          WARNINGS: ${{ steps.pssa.outputs.warning-count }}
          INFO: ${{ steps.pssa.outputs.info-count }}
          TOTAL: ${{ steps.pssa.outputs.total-issues }}
          LINE_COV: ${{ steps.coverage.outputs.line-coverage }}
          BRANCH_COV: ${{ steps.coverage.outputs.branch-coverage }}
        with:
          script: |
            const errors = parseInt(process.env.ERRORS || '0', 10);
            const warnings = parseInt(process.env.WARNINGS || '0', 10);
            const info = parseInt(process.env.INFO || '0', 10);
            const total = parseInt(process.env.TOTAL || '0', 10);
            const lineCov = parseFloat(process.env.LINE_COV || '0');
            const branchCov = parseFloat(process.env.BRANCH_COV || '0');

            let conclusion, summary;

            if (errors > 0) {
              conclusion = 'failure';
              summary = `âŒ ${errors} error(s), ${warnings} warning(s) found`;
            } else if (warnings > 10) {
              conclusion = 'neutral';
              summary = `âš ï¸ ${warnings} warning(s) found (no errors)`;
            } else {
              conclusion = 'success';
              summary = `âœ… Code quality passed (${total} total issues)`;
            }

            const text = `**PSScriptAnalyzer:**

            â€¢ Errors: ${errors}
            â€¢ Warnings: ${warnings}
            â€¢ Information: ${info}
            â€¢ Total Issues: ${total}

            **Code Coverage:**

            â€¢ Line Coverage: ${lineCov}%
            â€¢ Branch Coverage: ${branchCov}%`;

            await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'ğŸ” Static Analysis',
              head_sha: context.payload.pull_request.head.sha,
              status: 'completed',
              conclusion: conclusion,
              output: {
                title: summary,
                summary: summary,
                text: text
              }
            });

  # Consolidation job - runs after all parallel jobs complete
  consolidate-results:
    name: ğŸ“Š Consolidate & Report
    needs: [unit-tests-parallel, domain-tests-parallel, integration-tests-parallel, static-analysis]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: always()  # Run even if some tests failed

    steps:
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4

      - name: ğŸ”§ Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal

      - name: ğŸ“¥ Download All Test Results
        uses: actions/download-artifact@v4
        with:
          path: ./tests/results-consolidated

      - name: ğŸ“Š Consolidate Results
        id: consolidate
        shell: pwsh
        continue-on-error: true
        run: |
          Write-Host "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" -ForegroundColor Cyan
          Write-Host "ğŸ“Š Consolidating Parallel Test Results" -ForegroundColor Cyan
          Write-Host "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" -ForegroundColor Cyan

          $allResults = Get-ChildItem -Path "./tests/results-consolidated" -Recurse -Filter "*.xml"

          Write-Host "Found $($allResults.Count) test result files" -ForegroundColor Cyan

          $totalPassed = 0
          $totalFailed = 0
          $totalSkipped = 0

          foreach ($file in $allResults) {
            try {
              [xml]$xml = Get-Content $file.FullName
              $testRun = $xml.'test-results'
              if ($testRun) {
                $totalPassed += [int]$testRun.total - [int]$testRun.failures - [int]$testRun.ignored
                $totalFailed += [int]$testRun.failures
                $totalSkipped += [int]$testRun.ignored
              }
            } catch {
              Write-Host "âš ï¸  Could not parse $($file.Name): $_" -ForegroundColor Yellow
            }
          }

          Write-Host ""
          Write-Host "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" -ForegroundColor Cyan
          Write-Host "   PARALLEL TEST EXECUTION COMPLETE" -ForegroundColor Green
          Write-Host "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" -ForegroundColor Cyan
          Write-Host "   âœ… Passed:  $totalPassed" -ForegroundColor Green
          Write-Host "   âŒ Failed:  $totalFailed" -ForegroundColor $(if ($totalFailed -gt 0) { 'Red' } else { 'Green' })
          Write-Host "   â­ï¸  Skipped: $totalSkipped" -ForegroundColor Yellow
          Write-Host "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" -ForegroundColor Cyan
          Write-Host ""

          # Set outputs
          "total-passed=$totalPassed" >> $env:GITHUB_OUTPUT
          "total-failed=$totalFailed" >> $env:GITHUB_OUTPUT
          "total-skipped=$totalSkipped" >> $env:GITHUB_OUTPUT

          # Generate TestReport JSON for dashboard integration
          $timestamp = Get-Date -Format "yyyyMMdd-HHmmss"
          $testReport = @{
            TestType = "Parallel"
            GeneratedAt = (Get-Date -Format "yyyy-MM-ddTHH:mm:ss.fffZ")
            WorkflowRun = "${{ github.run_id }}"
            WorkflowURL = "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            WorkflowName = "Parallel Testing"
            Branch = "${{ github.ref_name }}"
            Commit = "${{ github.sha }}"
            CommitMessage = "${{ github.event.head_commit.message }}"
            Author = "${{ github.actor }}"

            # Test Results
            TotalCount = $totalPassed + $totalFailed + $totalSkipped
            PassedCount = $totalPassed
            FailedCount = $totalFailed
            SkippedCount = $totalSkipped
            FailureRate = if (($totalPassed + $totalFailed) -gt 0) { [math]::Round(($totalFailed / ($totalPassed + $totalFailed)) * 100, 2) } else { 0 }
            PassRate = if (($totalPassed + $totalFailed) -gt 0) { [math]::Round(($totalPassed / ($totalPassed + $totalFailed)) * 100, 2) } else { 0 }

            # Code Quality Metrics
            CodeQuality = @{
              PSScriptAnalyzer = @{
                TotalIssues = "${{ steps.pssa.outputs.total-issues || 0 }}"
                Errors = "${{ steps.pssa.outputs.error-count || 0 }}"
                Warnings = "${{ steps.pssa.outputs.warning-count || 0 }}"
                Information = "${{ steps.pssa.outputs.info-count || 0 }}"
              }
              Coverage = @{
                LineCoverage = "${{ steps.coverage.outputs.line-coverage || 0 }}"
                BranchCoverage = "${{ steps.coverage.outputs.branch-coverage || 0 }}"
              }
            }

            # Issue Tracking
            Issues = @{
              Total = "${{ steps.issues.outputs.total-issues || 0 }}"
              Bugs = "${{ steps.issues.outputs.bugs || 0 }}"
              Features = "${{ steps.issues.outputs.features || 0 }}"
              Critical = "${{ steps.issues.outputs.critical || 0 }}"
            }

            # Performance Metrics
            Performance = @{
              WorkflowDuration = "${{ steps.performance.outputs.workflow-duration || 0 }}"
              ParallelJobs = 17
              ResultFiles = $allResults.Count
            }

            # Health Assessment
            HealthScore = @{
              Overall = 0
              TestHealth = 0
              CodeQuality = 0
              IssueHealth = 0
            }

            TestResults = @{
              Summary = "Parallel execution across $($allResults.Count) test suites"
              ResultFiles = $allResults.Count
            }
          }

          # Calculate health scores (0-100)
          $testHealth = if ($testReport.TotalCount -gt 0) {
            [math]::Round($testReport.PassRate, 0)
          } else { 0 }

          $qualityHealth = 100
          $errors = [int]$testReport.CodeQuality.PSScriptAnalyzer.Errors
          $warnings = [int]$testReport.CodeQuality.PSScriptAnalyzer.Warnings
          if ($errors -gt 0) { $qualityHealth -= [math]::Min($errors * 5, 50) }
          if ($warnings -gt 0) { $qualityHealth -= [math]::Min($warnings, 30) }
          $qualityHealth = [math]::Max($qualityHealth, 0)

          $issueHealth = 100
          $bugs = [int]$testReport.Issues.Bugs
          $critical = [int]$testReport.Issues.Critical
          if ($bugs -gt 0) { $issueHealth -= [math]::Min($bugs * 2, 40) }
          if ($critical -gt 0) { $issueHealth -= [math]::Min($critical * 10, 50) }
          $issueHealth = [math]::Max($issueHealth, 0)

          $overallHealth = [math]::Round(($testHealth * 0.4 + $qualityHealth * 0.3 + $issueHealth * 0.3), 0)

          $testReport.HealthScore.Overall = $overallHealth
          $testReport.HealthScore.TestHealth = $testHealth
          $testReport.HealthScore.CodeQuality = $qualityHealth
          $testReport.HealthScore.IssueHealth = $issueHealth

          $testReportJson = $testReport | ConvertTo-Json -Depth 10

          # Save to tests/results for dashboard
          $resultsDir = "./tests/results"
          if (-not (Test-Path $resultsDir)) {
            New-Item -ItemType Directory -Path $resultsDir -Force | Out-Null
          }

          $reportPath = Join-Path $resultsDir "TestReport-Parallel-$timestamp.json"
          $testReportJson | Out-File -FilePath $reportPath -Encoding UTF8
          Write-Host "âœ… Generated comprehensive TestReport JSON: $reportPath" -ForegroundColor Green
          Write-Host "   Overall Health Score: $overallHealth/100" -ForegroundColor $(if ($overallHealth -ge 80) { 'Green' } elseif ($overallHealth -ge 60) { 'Yellow' } else { 'Red' })
          Write-Host "   Test Health: $testHealth/100" -ForegroundColor Cyan
          Write-Host "   Code Quality: $qualityHealth/100" -ForegroundColor Cyan
          Write-Host "   Issue Health: $issueHealth/100" -ForegroundColor Cyan

          # Mark as failed if tests failed (but continue-on-error allows subsequent steps)
          if ($totalFailed -gt 0) {
            Write-Host "âŒ Some tests failed - marking step as failed" -ForegroundColor Red
            exit 1
          }

      - name: ğŸ“Š Publish Test Report
        uses: dorny/test-reporter@v1
        if: always()
        continue-on-error: true
        with:
          name: 'âš¡ Parallel Test Results'
          path: './tests/results-consolidated/**/*.xml'
          reporter: 'java-junit'
          fail-on-error: false

      - name: ğŸ“‹ Create Status Check
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        env:
          PASSED: ${{ steps.consolidate.outputs.total-passed }}
          FAILED: ${{ steps.consolidate.outputs.total-failed }}
          SKIPPED: ${{ steps.consolidate.outputs.total-skipped }}
        with:
          script: |
            const passed = parseInt(process.env.PASSED || '0', 10);
            const failed = parseInt(process.env.FAILED || '0', 10);
            const skipped = parseInt(process.env.SKIPPED || '0', 10);
            const total = passed + failed + skipped;

            let conclusion, summary, text;

            if (failed === 0) {
              conclusion = 'success';
              summary = `âœ… All tests passed (${passed} passed, ${skipped} skipped)`;
              text = `Parallel test execution completed successfully.\n\n**Results:**\n- âœ… Passed: ${passed}\n- â­ï¸ Skipped: ${skipped}\n- Total: ${total}`;
            } else {
              conclusion = 'failure';
              summary = `âŒ ${failed} test(s) failed (${passed} passed, ${skipped} skipped)`;
              text = `Parallel test execution found failures.\n\n**Results:**\n- âœ… Passed: ${passed}\n- âŒ Failed: ${failed}\n- â­ï¸ Skipped: ${skipped}\n- Total: ${total}\n\n**Action Required:** Review failed tests and address issues.`;
            }

            await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'âš¡ Parallel Testing Results',
              head_sha: context.payload.pull_request.head.sha,
              status: 'completed',
              conclusion: conclusion,
              output: {
                title: summary,
                summary: summary,
                text: text
              }
            });

      - name: ğŸ“‹ Collect Check Statuses for Dashboard
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Get all checks for this PR
            const { data: checkRuns } = await github.rest.checks.listForRef({
              owner: context.repo.owner,
              repo: context.repo.repo,
              ref: context.payload.pull_request.head.sha,
              per_page: 100
            });

            // Filter for parallel testing checks only
            const parallelTestChecks = checkRuns.check_runs.filter(check =>
              check.name.includes('Unit Tests') ||
              check.name.includes('Domain Tests') ||
              check.name.includes('Integration Tests') ||
              check.name.includes('Static Analysis') ||
              check.name.includes('Parallel Testing Results')
            );

            // Organize checks by type
            const checkSummary = {
              timestamp: new Date().toISOString(),
              workflow_run: '${{ github.run_id }}',
              pr_number: context.payload.pull_request.number,
              commit_sha: context.payload.pull_request.head.sha,
              total_checks: parallelTestChecks.length,
              checks: {
                unit_tests: [],
                domain_tests: [],
                integration_tests: [],
                static_analysis: [],
                overall: []
              },
              summary: {
                total: parallelTestChecks.length,
                success: 0,
                failure: 0,
                neutral: 0,
                skipped: 0
              }
            };

            // Process each check
            for (const check of parallelTestChecks) {
              const checkData = {
                name: check.name,
                conclusion: check.conclusion,
                status: check.status,
                started_at: check.started_at,
                completed_at: check.completed_at,
                html_url: check.html_url,
                output_title: check.output?.title || '',
                output_summary: check.output?.summary || ''
              };

              // Count by conclusion
              if (check.conclusion === 'success') checkSummary.summary.success++;
              else if (check.conclusion === 'failure') checkSummary.summary.failure++;
              else if (check.conclusion === 'neutral') checkSummary.summary.neutral++;
              else if (check.conclusion === 'skipped') checkSummary.summary.skipped++;

              // Categorize by type
              if (check.name.includes('Unit Tests')) {
                checkSummary.checks.unit_tests.push(checkData);
              } else if (check.name.includes('Domain Tests')) {
                checkSummary.checks.domain_tests.push(checkData);
              } else if (check.name.includes('Integration Tests')) {
                checkSummary.checks.integration_tests.push(checkData);
              } else if (check.name.includes('Static Analysis')) {
                checkSummary.checks.static_analysis.push(checkData);
              } else if (check.name.includes('Parallel Testing Results')) {
                checkSummary.checks.overall.push(checkData);
              }
            }

            // Save to reports directory for dashboard
            const reportsDir = './reports';
            if (!fs.existsSync(reportsDir)) {
              fs.mkdirSync(reportsDir, { recursive: true });
            }

            const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, -5);
            const checkStatusPath = path.join(reportsDir, `ParallelTestChecks-${timestamp}.json`);
            fs.writeFileSync(checkStatusPath, JSON.stringify(checkSummary, null, 2));

            core.info(`âœ… Saved check statuses to ${checkStatusPath}`);
            core.info(`   Total checks: ${checkSummary.total_checks}`);
            core.info(`   Success: ${checkSummary.summary.success}`);
            core.info(`   Failure: ${checkSummary.summary.failure}`);
            core.info(`   Neutral: ${checkSummary.summary.neutral}`);

      - name: ğŸ’¬ Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        env:
          PASSED: ${{ steps.consolidate.outputs.total-passed }}
          FAILED: ${{ steps.consolidate.outputs.total-failed }}
          SKIPPED: ${{ steps.consolidate.outputs.total-skipped }}
        with:
          script: |
            const script = require('./.github/scripts/generate-test-comment.js');
            const mockCore = {
              getInput: (name) => {
                const inputs = {
                  'passed': process.env.PASSED,
                  'failed': process.env.FAILED,
                  'skipped': process.env.SKIPPED
                };
                return inputs[name] || '';
              }
            };
            return await script({github, context, core: mockCore});

      - name: ğŸ“Š Generate Dashboard
        if: always()
        shell: pwsh
        continue-on-error: true
        run: |
          Write-Host "ğŸ“Š Generating comprehensive dashboard..." -ForegroundColor Cyan
          ./library/automation-scripts/0512_Generate-Dashboard.ps1 -Format All

      - name: ğŸ’¾ Commit Updated Persisted Reports
        if: always() && github.event_name != 'pull_request'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Add only the persisted reports that should be tracked
          git add reports/dashboard.html reports/dashboard.md reports/dashboard.json
          git add reports/psscriptanalyzer-fast-results.json
          git add reports/README.md reports/index.md
          git add reports/metrics-history/*.json
          
          # Commit if there are changes
          if ! git diff --staged --quiet; then
            git commit -m "chore: update persisted reports - $(date -u +"%Y-%m-%d %H:%M UTC") [skip ci]"
            git push origin HEAD:${{ github.ref_name }}
            echo "âœ… Persisted reports committed and pushed"
          else
            echo "â„¹ï¸ No changes to persisted reports"
          fi

      - name: ğŸ“¤ Upload Dashboard & Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: parallel-test-dashboard
          path: |
            reports/**/*.html
            reports/**/*.json
            reports/**/*.md
            tests/results/TestReport-Parallel-*.json
          retention-days: 30
          if-no-files-found: warn

      - name: âœ… Final Status Check
        if: always()
        shell: pwsh
        run: |
          $failed = "${{ steps.consolidate.outputs.total-failed }}"
          if ($failed -and $failed -gt 0) {
            Write-Host "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" -ForegroundColor Red
            Write-Host "âŒ WORKFLOW FAILED: $failed test(s) failed" -ForegroundColor Red
            Write-Host "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" -ForegroundColor Red
            Write-Host ""
            Write-Host "This failure is informational and does NOT block merging." -ForegroundColor Yellow
            Write-Host "Please review failed tests and address issues before merging." -ForegroundColor Yellow
            Write-Host ""
            exit 1
          } else {
            Write-Host "âœ… All checks passed!" -ForegroundColor Green
          }

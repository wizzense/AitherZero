---
name: âš¡ Parallel Testing (High Performance)

# High-performance parallel test execution across multiple runners
# Tests are split by range and executed concurrently, then consolidated

on:
  push:
    branches: [main, develop, dev, dev-staging]
    paths:
      - 'domains/**'
      - 'automation-scripts/**'
      - 'tests/**'
      - '**.ps1'
      - '**.psm1'
  pull_request:
    branches: [main, develop, dev, dev-staging]
  workflow_dispatch:
    inputs:
      test_filter:
        description: 'Test filter (all, unit, integration, domains)'
        type: choice
        options:
          - all
          - unit
          - integration
          - domains
        default: 'all'

permissions:
  contents: read
  checks: write
  pull-requests: write

# Allow parallel execution across different test suites
concurrency:
  group: parallel-tests-${{ github.ref }}-${{ github.run_number }}
  cancel-in-progress: false  # Keep running - we want parallel jobs

env:
  AITHERZERO_CI: true
  AITHERZERO_NONINTERACTIVE: true

jobs:
  # Fast preparation job - sets up for parallel execution
  prepare:
    name: ğŸ¯ Prepare Test Matrix
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      unit-ranges: ${{ steps.matrix.outputs.unit-ranges }}
      domain-modules: ${{ steps.matrix.outputs.domain-modules }}
      integration-suites: ${{ steps.matrix.outputs.integration-suites }}
    
    steps:
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ğŸ”§ Quick Bootstrap
        shell: pwsh
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal
      
      - name: ğŸ¯ Generate Test Matrix
        id: matrix
        shell: pwsh
        run: |
          # Unit test ranges (by script number ranges)
          $unitRanges = @(
            '0000-0099',
            '0100-0199',
            '0200-0299',
            '0400-0499',
            '0500-0599',
            '0700-0799',
            '0800-0899',
            '0900-0999'
          )
          
          # Domain modules - ONLY modules that have test directories
          $domainModules = @(
            'configuration',
            'documentation',
            'infrastructure',
            'security',
            'testing',
            'utilities'
          )
          
          # Integration test suites - ONLY suites that exist
          $integrationSuites = @(
            'automation-scripts'
          )
          
          # Output as JSON arrays for matrix
          $unitRangesJson = $unitRanges | ConvertTo-Json -Compress
          $domainModulesJson = $domainModules | ConvertTo-Json -Compress
          $integrationSuitesJson = $integrationSuites | ConvertTo-Json -Compress
          
          "unit-ranges=$unitRangesJson" >> $env:GITHUB_OUTPUT
          "domain-modules=$domainModulesJson" >> $env:GITHUB_OUTPUT
          "integration-suites=$integrationSuitesJson" >> $env:GITHUB_OUTPUT
          
          Write-Host "âœ… Test matrix generated" -ForegroundColor Green
          Write-Host "   Unit ranges: $($unitRanges.Count)" -ForegroundColor Cyan
          Write-Host "   Domain modules: $($domainModules.Count)" -ForegroundColor Cyan
          Write-Host "   Integration suites: $($integrationSuites.Count)" -ForegroundColor Cyan

  # Parallel unit tests by script number range
  unit-tests-parallel:
    name: ğŸ§ª Unit Tests [${{ matrix.range }}]
    needs: prepare
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: github.event.inputs.test_filter == 'all' || github.event.inputs.test_filter == 'unit' || github.event.inputs.test_filter == ''
    
    strategy:
      fail-fast: false  # Continue all jobs even if one fails
      max-parallel: 8   # Run up to 8 jobs simultaneously
      matrix:
        range: ${{ fromJson(needs.prepare.outputs.unit-ranges) }}
    
    steps:
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ğŸ”§ Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal
      
      - name: ğŸ§ª Run Unit Tests [${{ matrix.range }}]
        id: run-tests
        shell: pwsh
        continue-on-error: true
        timeout-minutes: 5
        run: |
          Write-Host "âš¡ Running unit tests for range: ${{ matrix.range }}" -ForegroundColor Cyan
          
          $testPath = "./tests/unit/automation-scripts/${{ matrix.range }}"
          
          if (Test-Path $testPath) {
            $config = New-PesterConfiguration
            $config.Run.Path = $testPath
            $config.Run.Exit = $false
            $config.Run.PassThru = $true
            $config.Output.Verbosity = 'Normal'
            $config.TestResult.Enabled = $true
            $config.TestResult.OutputPath = "./tests/results/UnitTests-${{ matrix.range }}.xml"
            $config.TestResult.OutputFormat = 'NUnitXml'
            
            $result = Invoke-Pester -Configuration $config
            
            Write-Host "âœ… Tests completed for ${{ matrix.range }}" -ForegroundColor Green
            Write-Host "   Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "   Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })
            Write-Host "   Skipped: $($result.SkippedCount)" -ForegroundColor Yellow
            
            # Mark step as failed but don't exit (continue-on-error allows continuation)
            if ($result.FailedCount -gt 0) {
              Write-Host "##vso[task.complete result=Failed;]Tests failed"
              exit 1
            }
          } else {
            Write-Host "â­ï¸  No tests found for range ${{ matrix.range }}" -ForegroundColor Yellow
          }
      
      - name: ğŸ“Š Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-tests-${{ matrix.range }}
          path: ./tests/results/UnitTests-${{ matrix.range }}.xml
          retention-days: 7
          if-no-files-found: warn

  # Parallel domain module tests
  domain-tests-parallel:
    name: ğŸ—ï¸ Domain Tests [${{ matrix.module }}]
    needs: prepare
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: github.event.inputs.test_filter == 'all' || github.event.inputs.test_filter == 'domains' || github.event.inputs.test_filter == ''
    
    strategy:
      fail-fast: false
      max-parallel: 8
      matrix:
        module: ${{ fromJson(needs.prepare.outputs.domain-modules) }}
    
    steps:
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ğŸ”§ Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal
      
      - name: ğŸ—ï¸ Run Domain Tests [${{ matrix.module }}]
        id: run-tests
        shell: pwsh
        continue-on-error: true
        timeout-minutes: 5
        run: |
          Write-Host "âš¡ Running domain tests for: ${{ matrix.module }}" -ForegroundColor Cyan
          
          $testPath = "./tests/domains/${{ matrix.module }}"
          
          if (Test-Path $testPath) {
            $config = New-PesterConfiguration
            $config.Run.Path = $testPath
            $config.Run.Exit = $false
            $config.Run.PassThru = $true
            $config.Output.Verbosity = 'Normal'
            $config.TestResult.Enabled = $true
            $config.TestResult.OutputPath = "./tests/results/DomainTests-${{ matrix.module }}.xml"
            $config.TestResult.OutputFormat = 'NUnitXml'
            
            $result = Invoke-Pester -Configuration $config
            
            Write-Host "âœ… Tests completed for ${{ matrix.module }}" -ForegroundColor Green
            Write-Host "   Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "   Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })
            Write-Host "   Skipped: $($result.SkippedCount)" -ForegroundColor Yellow
            
            # Mark step as failed but don't exit (continue-on-error allows continuation)
            if ($result.FailedCount -gt 0) {
              Write-Host "##vso[task.complete result=Failed;]Tests failed"
              exit 1
            }
          } else {
            Write-Host "â­ï¸  No tests found for module ${{ matrix.module }}" -ForegroundColor Yellow
          }
      
      - name: ğŸ“Š Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: domain-tests-${{ matrix.module }}
          path: ./tests/results/DomainTests-${{ matrix.module }}.xml
          retention-days: 7
          if-no-files-found: warn

  # Parallel integration tests
  integration-tests-parallel:
    name: ğŸ”— Integration Tests [${{ matrix.suite }}]
    needs: prepare
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event.inputs.test_filter == 'all' || github.event.inputs.test_filter == 'integration' || github.event.inputs.test_filter == ''
    
    strategy:
      fail-fast: false
      max-parallel: 3
      matrix:
        suite: ${{ fromJson(needs.prepare.outputs.integration-suites) }}
    
    steps:
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ğŸ”§ Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal
      
      - name: ğŸ”— Run Integration Tests [${{ matrix.suite }}]
        id: run-tests
        shell: pwsh
        continue-on-error: true
        timeout-minutes: 10
        run: |
          Write-Host "âš¡ Running integration tests for: ${{ matrix.suite }}" -ForegroundColor Cyan
          
          $testPath = "./tests/integration/${{ matrix.suite }}"
          
          if (Test-Path $testPath) {
            $config = New-PesterConfiguration
            $config.Run.Path = $testPath
            $config.Run.Exit = $false
            $config.Run.PassThru = $true
            $config.Output.Verbosity = 'Normal'
            $config.TestResult.Enabled = $true
            $config.TestResult.OutputPath = "./tests/results/IntegrationTests-${{ matrix.suite }}.xml"
            $config.TestResult.OutputFormat = 'NUnitXml'
            
            $result = Invoke-Pester -Configuration $config
            
            Write-Host "âœ… Tests completed for ${{ matrix.suite }}" -ForegroundColor Green
            Write-Host "   Passed: $($result.PassedCount)" -ForegroundColor Green
            Write-Host "   Failed: $($result.FailedCount)" -ForegroundColor $(if ($result.FailedCount -gt 0) { 'Red' } else { 'Green' })
            Write-Host "   Skipped: $($result.SkippedCount)" -ForegroundColor Yellow
            
            # Mark step as failed but don't exit (continue-on-error allows continuation)
            if ($result.FailedCount -gt 0) {
              Write-Host "##vso[task.complete result=Failed;]Tests failed"
              exit 1
            }
          } else {
            Write-Host "â­ï¸  No tests found for suite ${{ matrix.suite }}" -ForegroundColor Yellow
          }
      
      - name: ğŸ“Š Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-tests-${{ matrix.suite }}
          path: ./tests/results/IntegrationTests-${{ matrix.suite }}.xml
          retention-days: 7
          if-no-files-found: warn

  # Static analysis (runs in parallel with tests)
  static-analysis:
    name: ğŸ” Static Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: github.event.inputs.test_filter == 'all' || github.event.inputs.test_filter == ''
    
    steps:
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ğŸ”§ Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal
      
      - name: ğŸ” Syntax Validation
        shell: pwsh
        timeout-minutes: 2
        run: |
          Write-Host "âš¡ Running syntax validation" -ForegroundColor Cyan
          ./automation-scripts/0407_Validate-Syntax.ps1 -All
      
      - name: ğŸ” PSScriptAnalyzer
        id: pssa
        shell: pwsh
        timeout-minutes: 5
        continue-on-error: true
        run: |
          Write-Host "âš¡ Running PSScriptAnalyzer" -ForegroundColor Cyan
          ./automation-scripts/0404_Run-PSScriptAnalyzer.ps1
          
          # Capture summary for later use
          $summaryPath = Get-ChildItem "./tests/analysis" -Filter "PSScriptAnalyzer-Summary-*.json" -ErrorAction SilentlyContinue | 
            Sort-Object LastWriteTime -Descending | Select-Object -First 1
          
          if ($summaryPath) {
            $summary = Get-Content $summaryPath.FullName | ConvertFrom-Json
            "error-count=$($summary.BySeverity.Error)" >> $env:GITHUB_OUTPUT
            "warning-count=$($summary.BySeverity.Warning)" >> $env:GITHUB_OUTPUT
            "info-count=$($summary.BySeverity.Information)" >> $env:GITHUB_OUTPUT
            "total-issues=$($summary.TotalIssues)" >> $env:GITHUB_OUTPUT
          }
      
      - name: ğŸ“Š Code Coverage Analysis
        id: coverage
        shell: pwsh
        timeout-minutes: 3
        continue-on-error: true
        run: |
          Write-Host "âš¡ Analyzing code coverage from test runs..." -ForegroundColor Cyan
          
          # Check for coverage files from test runs
          $coverageFiles = Get-ChildItem "./tests/coverage" -Filter "Coverage-*.xml" -ErrorAction SilentlyContinue
          
          if ($coverageFiles) {
            Write-Host "Found $($coverageFiles.Count) coverage file(s)" -ForegroundColor Cyan
            
            # Parse latest coverage file
            $latestCoverage = $coverageFiles | Sort-Object LastWriteTime -Descending | Select-Object -First 1
            [xml]$covXml = Get-Content $latestCoverage.FullName
            
            $lineRate = [math]::Round([double]$covXml.coverage.'line-rate' * 100, 2)
            $branchRate = [math]::Round([double]$covXml.coverage.'branch-rate' * 100, 2)
            
            "line-coverage=$lineRate" >> $env:GITHUB_OUTPUT
            "branch-coverage=$branchRate" >> $env:GITHUB_OUTPUT
            
            Write-Host "âœ… Line Coverage: $lineRate%" -ForegroundColor Green
            Write-Host "âœ… Branch Coverage: $branchRate%" -ForegroundColor Green
          } else {
            Write-Host "âš ï¸  No coverage files found" -ForegroundColor Yellow
            "line-coverage=0" >> $env:GITHUB_OUTPUT
            "branch-coverage=0" >> $env:GITHUB_OUTPUT
          }
      
      - name: ğŸ“Š Upload Analysis Results
        if: always() && hashFiles('./tests/analysis/*') != ''
        uses: actions/upload-artifact@v4
        with:
          name: static-analysis
          path: ./tests/analysis/*
          retention-days: 7
      
      - name: ğŸ› Collect Open Issues
        id: issues
        if: always()
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const { data: issues } = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              per_page: 100
            });
            
            const bugs = issues.filter(i => i.labels.some(l => l.name === 'bug'));
            const features = issues.filter(i => i.labels.some(l => l.name === 'enhancement' || l.name === 'feature'));
            const critical = issues.filter(i => i.labels.some(l => l.name === 'critical' || l.name === 'priority: high'));
            
            core.setOutput('total-issues', issues.length);
            core.setOutput('bugs', bugs.length);
            core.setOutput('features', features.length);
            core.setOutput('critical', critical.length);
            
            // Save detailed issue data
            const fs = require('fs');
            const issuesDir = './reports';
            if (!fs.existsSync(issuesDir)) {
              fs.mkdirSync(issuesDir, { recursive: true });
            }
            
            const issueData = {
              timestamp: new Date().toISOString(),
              total: issues.length,
              bugs: bugs.length,
              features: features.length,
              critical: critical.length,
              issues: issues.map(i => ({
                number: i.number,
                title: i.title,
                state: i.state,
                labels: i.labels.map(l => l.name),
                created_at: i.created_at,
                updated_at: i.updated_at,
                url: i.html_url
              }))
            };
            
            fs.writeFileSync('./reports/open-issues.json', JSON.stringify(issueData, null, 2));
            console.log(`âœ… Collected ${issues.length} open issues`);
      
      - name: â±ï¸ Performance Metrics
        id: performance
        if: always()
        shell: pwsh
        continue-on-error: true
        run: |
          Write-Host "âš¡ Collecting performance metrics..." -ForegroundColor Cyan
          
          # Calculate workflow execution time
          $startTime = "${{ github.event.workflow_run.created_at || github.event.head_commit.timestamp }}"
          if ($startTime) {
            $start = [DateTime]::Parse($startTime)
            $duration = (Get-Date) - $start
            $durationMinutes = [math]::Round($duration.TotalMinutes, 2)
            "workflow-duration=$durationMinutes" >> $env:GITHUB_OUTPUT
            Write-Host "Workflow duration: $durationMinutes minutes" -ForegroundColor Cyan
          }
          
          # Collect performance data from logs if available
          $perfData = @{
            Timestamp = (Get-Date -Format "yyyy-MM-ddTHH:mm:ss.fffZ")
            WorkflowRun = "${{ github.run_id }}"
            Metrics = @{
              TestExecutionTime = $durationMinutes
              ParallelJobs = 17  # Total jobs in parallel execution
              AverageJobDuration = 0
            }
          }
          
          $perfData | ConvertTo-Json -Depth 10 | Out-File -FilePath "./reports/performance-metrics.json" -Encoding UTF8
          Write-Host "âœ… Performance metrics collected" -ForegroundColor Green

  # Consolidation job - runs after all parallel jobs complete
  consolidate-results:
    name: ğŸ“Š Consolidate & Report
    needs: [unit-tests-parallel, domain-tests-parallel, integration-tests-parallel, static-analysis]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: always()  # Run even if some tests failed
    
    steps:
      - name: ğŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ğŸ”§ Bootstrap
        shell: pwsh
        timeout-minutes: 3
        run: |
          ./bootstrap.ps1 -Mode New -InstallProfile Minimal
      
      - name: ğŸ“¥ Download All Test Results
        uses: actions/download-artifact@v4
        with:
          path: ./tests/results-consolidated
      
      - name: ğŸ“Š Consolidate Results
        id: consolidate
        shell: pwsh
        continue-on-error: true
        run: |
          Write-Host "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" -ForegroundColor Cyan
          Write-Host "ğŸ“Š Consolidating Parallel Test Results" -ForegroundColor Cyan
          Write-Host "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" -ForegroundColor Cyan
          
          $allResults = Get-ChildItem -Path "./tests/results-consolidated" -Recurse -Filter "*.xml"
          
          Write-Host "Found $($allResults.Count) test result files" -ForegroundColor Cyan
          
          $totalPassed = 0
          $totalFailed = 0
          $totalSkipped = 0
          
          foreach ($file in $allResults) {
            try {
              [xml]$xml = Get-Content $file.FullName
              $testRun = $xml.'test-results'
              if ($testRun) {
                $totalPassed += [int]$testRun.total - [int]$testRun.failures - [int]$testRun.ignored
                $totalFailed += [int]$testRun.failures
                $totalSkipped += [int]$testRun.ignored
              }
            } catch {
              Write-Host "âš ï¸  Could not parse $($file.Name): $_" -ForegroundColor Yellow
            }
          }
          
          Write-Host ""
          Write-Host "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" -ForegroundColor Cyan
          Write-Host "   PARALLEL TEST EXECUTION COMPLETE" -ForegroundColor Green
          Write-Host "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" -ForegroundColor Cyan
          Write-Host "   âœ… Passed:  $totalPassed" -ForegroundColor Green
          Write-Host "   âŒ Failed:  $totalFailed" -ForegroundColor $(if ($totalFailed -gt 0) { 'Red' } else { 'Green' })
          Write-Host "   â­ï¸  Skipped: $totalSkipped" -ForegroundColor Yellow
          Write-Host "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" -ForegroundColor Cyan
          Write-Host ""
          
          # Set outputs
          "total-passed=$totalPassed" >> $env:GITHUB_OUTPUT
          "total-failed=$totalFailed" >> $env:GITHUB_OUTPUT
          "total-skipped=$totalSkipped" >> $env:GITHUB_OUTPUT
          
          # Generate TestReport JSON for dashboard integration
          $timestamp = Get-Date -Format "yyyyMMdd-HHmmss"
          $testReport = @{
            TestType = "Parallel"
            GeneratedAt = (Get-Date -Format "yyyy-MM-ddTHH:mm:ss.fffZ")
            WorkflowRun = "${{ github.run_id }}"
            WorkflowURL = "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            WorkflowName = "Parallel Testing"
            Branch = "${{ github.ref_name }}"
            Commit = "${{ github.sha }}"
            CommitMessage = "${{ github.event.head_commit.message }}"
            Author = "${{ github.actor }}"
            
            # Test Results
            TotalCount = $totalPassed + $totalFailed + $totalSkipped
            PassedCount = $totalPassed
            FailedCount = $totalFailed
            SkippedCount = $totalSkipped
            FailureRate = if (($totalPassed + $totalFailed) -gt 0) { [math]::Round(($totalFailed / ($totalPassed + $totalFailed)) * 100, 2) } else { 0 }
            PassRate = if (($totalPassed + $totalFailed) -gt 0) { [math]::Round(($totalPassed / ($totalPassed + $totalFailed)) * 100, 2) } else { 0 }
            
            # Code Quality Metrics
            CodeQuality = @{
              PSScriptAnalyzer = @{
                TotalIssues = "${{ steps.pssa.outputs.total-issues || 0 }}"
                Errors = "${{ steps.pssa.outputs.error-count || 0 }}"
                Warnings = "${{ steps.pssa.outputs.warning-count || 0 }}"
                Information = "${{ steps.pssa.outputs.info-count || 0 }}"
              }
              Coverage = @{
                LineCoverage = "${{ steps.coverage.outputs.line-coverage || 0 }}"
                BranchCoverage = "${{ steps.coverage.outputs.branch-coverage || 0 }}"
              }
            }
            
            # Issue Tracking
            Issues = @{
              Total = "${{ steps.issues.outputs.total-issues || 0 }}"
              Bugs = "${{ steps.issues.outputs.bugs || 0 }}"
              Features = "${{ steps.issues.outputs.features || 0 }}"
              Critical = "${{ steps.issues.outputs.critical || 0 }}"
            }
            
            # Performance Metrics
            Performance = @{
              WorkflowDuration = "${{ steps.performance.outputs.workflow-duration || 0 }}"
              ParallelJobs = 17
              ResultFiles = $allResults.Count
            }
            
            # Health Assessment
            HealthScore = @{
              Overall = 0
              TestHealth = 0
              CodeQuality = 0
              IssueHealth = 0
            }
            
            TestResults = @{
              Summary = "Parallel execution across $($allResults.Count) test suites"
              ResultFiles = $allResults.Count
            }
          }
          
          # Calculate health scores (0-100)
          $testHealth = if ($testReport.TotalCount -gt 0) { 
            [math]::Round($testReport.PassRate, 0) 
          } else { 0 }
          
          $qualityHealth = 100
          $errors = [int]$testReport.CodeQuality.PSScriptAnalyzer.Errors
          $warnings = [int]$testReport.CodeQuality.PSScriptAnalyzer.Warnings
          if ($errors -gt 0) { $qualityHealth -= [math]::Min($errors * 5, 50) }
          if ($warnings -gt 0) { $qualityHealth -= [math]::Min($warnings, 30) }
          $qualityHealth = [math]::Max($qualityHealth, 0)
          
          $issueHealth = 100
          $bugs = [int]$testReport.Issues.Bugs
          $critical = [int]$testReport.Issues.Critical
          if ($bugs -gt 0) { $issueHealth -= [math]::Min($bugs * 2, 40) }
          if ($critical -gt 0) { $issueHealth -= [math]::Min($critical * 10, 50) }
          $issueHealth = [math]::Max($issueHealth, 0)
          
          $overallHealth = [math]::Round(($testHealth * 0.4 + $qualityHealth * 0.3 + $issueHealth * 0.3), 0)
          
          $testReport.HealthScore.Overall = $overallHealth
          $testReport.HealthScore.TestHealth = $testHealth
          $testReport.HealthScore.CodeQuality = $qualityHealth
          $testReport.HealthScore.IssueHealth = $issueHealth
          
          $testReportJson = $testReport | ConvertTo-Json -Depth 10
          
          # Save to tests/results for dashboard
          $resultsDir = "./tests/results"
          if (-not (Test-Path $resultsDir)) {
            New-Item -ItemType Directory -Path $resultsDir -Force | Out-Null
          }
          
          $reportPath = Join-Path $resultsDir "TestReport-Parallel-$timestamp.json"
          $testReportJson | Out-File -FilePath $reportPath -Encoding UTF8
          Write-Host "âœ… Generated comprehensive TestReport JSON: $reportPath" -ForegroundColor Green
          Write-Host "   Overall Health Score: $overallHealth/100" -ForegroundColor $(if ($overallHealth -ge 80) { 'Green' } elseif ($overallHealth -ge 60) { 'Yellow' } else { 'Red' })
          Write-Host "   Test Health: $testHealth/100" -ForegroundColor Cyan
          Write-Host "   Code Quality: $qualityHealth/100" -ForegroundColor Cyan
          Write-Host "   Issue Health: $issueHealth/100" -ForegroundColor Cyan
          
          # Mark as failed if tests failed (but continue-on-error allows subsequent steps)
          if ($totalFailed -gt 0) {
            Write-Host "âŒ Some tests failed - marking step as failed" -ForegroundColor Red
            exit 1
          }
      
      - name: ğŸ“Š Publish Test Report
        uses: dorny/test-reporter@v1
        if: always()
        continue-on-error: true
        with:
          name: 'âš¡ Parallel Test Results'
          path: './tests/results-consolidated/**/*.xml'
          reporter: 'dotnet-trx'
          fail-on-error: false
      
      - name: ğŸ’¬ Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        env:
          PASSED: ${{ steps.consolidate.outputs.total-passed }}
          FAILED: ${{ steps.consolidate.outputs.total-failed }}
          SKIPPED: ${{ steps.consolidate.outputs.total-skipped }}
        with:
          script: |
            const script = require('./.github/scripts/generate-test-comment.js');
            const mockCore = {
              getInput: (name) => {
                const inputs = {
                  'passed': process.env.PASSED,
                  'failed': process.env.FAILED,
                  'skipped': process.env.SKIPPED
                };
                return inputs[name] || '';
              }
            };
            return await script({github, context, core: mockCore});
      
      - name: ğŸ“Š Generate Dashboard
        if: always()
        shell: pwsh
        continue-on-error: true
        run: |
          Write-Host "ğŸ“Š Generating comprehensive dashboard..." -ForegroundColor Cyan
          ./automation-scripts/0512_Generate-Dashboard.ps1 -Format All
      
      - name: ğŸ“¤ Upload Dashboard & Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: parallel-test-dashboard
          path: |
            reports/**/*.html
            reports/**/*.json
            reports/**/*.md
            tests/results/TestReport-Parallel-*.json
          retention-days: 30
          if-no-files-found: warn
      
      - name: âœ… Final Status Check
        if: always()
        shell: pwsh
        run: |
          $failed = "${{ steps.consolidate.outputs.total-failed }}"
          if ($failed -and $failed -gt 0) {
            Write-Host "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" -ForegroundColor Red
            Write-Host "âŒ WORKFLOW FAILED: $failed test(s) failed" -ForegroundColor Red
            Write-Host "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" -ForegroundColor Red
            Write-Host ""
            Write-Host "This failure is informational and does NOT block merging." -ForegroundColor Yellow
            Write-Host "Please review failed tests and address issues before merging." -ForegroundColor Yellow
            Write-Host ""
            exit 1
          } else {
            Write-Host "âœ… All checks passed!" -ForegroundColor Green
          }

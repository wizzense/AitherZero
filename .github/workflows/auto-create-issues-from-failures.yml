---
name: üö® Auto-Create Issues from Test Failures

# Automatically analyze test results and create targeted GitHub issues
on:
  # Re-enabled with improved deduplication
  workflow_run:
    workflows: ["Quality Validation", "PR Validation"]
    types: [completed]
  # Re-enabled with hash-based deduplication to prevent duplicates
  schedule:
    # Run daily to catch any missed failures
    - cron: '0 7 * * *'  # 7 AM UTC daily
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Dry run - preview issues without creating them'
        type: boolean
        default: false
      skip_deduplication:
        description: 'Skip deduplication checks (testing only)'
        type: boolean
        default: false

permissions:
  contents: read
  issues: write
  actions: read
  checks: write

jobs:
  analyze-test-failures:
    name: üîç Analyze Test Failures
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'schedule' ||
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'workflow_run'
    
    outputs:
      has-test-failures: ${{ steps.analysis.outputs.has-test-failures }}
      has-code-quality-issues: ${{ steps.analysis.outputs.has-code-quality-issues }}
      total-failures: ${{ steps.analysis.outputs.total-failures }}
    
    steps:
      - name: üì• Checkout Repository
        uses: actions/checkout@v4
        
      - name: üìä Download Test Artifacts
        if: github.event.workflow_run
        uses: dawidd6/action-download-artifact@v6
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          run_id: ${{ github.event.workflow_run.id }}
          path: ./test-artifacts
        continue-on-error: true
        
      - name: üîç Analyze Test Results
        id: analysis
        shell: pwsh
        run: |
          Write-Host "üîç Analyzing test results for failures..." -ForegroundColor Cyan
          
          # Helper function to generate unique hash for test failures
          function Get-FailureHash {
              param($TestName, $File, $ErrorMessage)
              $hashInput = "$TestName|$File|$($ErrorMessage -replace '\d+', 'N')"  # Normalize error messages with numbers
              $bytes = [System.Text.Encoding]::UTF8.GetBytes($hashInput)
              $hasher = [System.Security.Cryptography.SHA256]::Create()
              $hashBytes = $hasher.ComputeHash($bytes)
              return [System.BitConverter]::ToString($hashBytes).Replace('-', '').Substring(0, 16)
          }
          
          $testFailures = @()
          $codeQualityIssues = @()
          
          # Analyze test report files (NEW FORMAT)
          $testReportFiles = Get-ChildItem -Path "./reports", "./test-artifacts", "./tests/results" -Filter "TestReport*.json" -Recurse -ErrorAction SilentlyContinue
          
          Write-Host "Found $($testReportFiles.Count) test report files" -ForegroundColor Yellow
          
          foreach ($reportFile in $testReportFiles) {
            try {
              Write-Host "  Processing: $($reportFile.Name)" -ForegroundColor Gray
              $report = Get-Content $reportFile.FullName | ConvertFrom-Json
              
              # Check for test results in the expected structure
              $failedCount = 0
              if ($report.TestResults -and $report.TestResults.Summary) {
                $failedCount = $report.TestResults.Summary.Failed
              } elseif ($report.FailedCount) {
                $failedCount = $report.FailedCount
              }
              
              if ($failedCount -gt 0) {
                Write-Host "  Found $failedCount test failures" -ForegroundColor Red
                
                # Extract failure details from Details array
                if ($report.TestResults.Details) {
                  foreach ($result in $report.TestResults.Details) {
                    if ($result.Result -eq 'Failed') {
                      $testName = $result.ExpandedName ?? "Unknown Test"
                      $errorMsg = if ($result.ErrorRecord) { 
                        $result.ErrorRecord.Exception.Message 
                      } else { 
                        "Test failed" 
                      }
                      $file = if ($result.ScriptBlock) { 
                        $result.ScriptBlock.File 
                      } else { 
                        "Unknown" 
                      }
                      
                      $failureHash = Get-FailureHash -TestName $testName -File $file -ErrorMessage $errorMsg
                      
                      $testFailures += @{
                        Hash = $failureHash
                        TestName = $testName
                        ErrorMessage = $errorMsg
                        File = $file
                        Line = if ($result.ScriptBlock) { $result.ScriptBlock.StartPosition.Line } else { 0 }
                        TestType = $report.TestType ?? 'Unknown'
                      }
                    }
                  }
                }
              }
            } catch {
              Write-Warning "Failed to parse report: $($reportFile.Name) - $_"
            }
          }
          
          # Analyze PSScriptAnalyzer results (NEW FORMAT)
          $psAnalysisFiles = Get-ChildItem -Path "./reports", "./test-artifacts", "./tests/results" -Filter "TestReport-PSScriptAnalyzer*.json" -Recurse -ErrorAction SilentlyContinue
          
          Write-Host "Found $($psAnalysisFiles.Count) PSScriptAnalyzer report files" -ForegroundColor Yellow
          
          Write-Host "Found $($psAnalysisFiles.Count) PSScriptAnalyzer report files" -ForegroundColor Yellow
          
          foreach ($analysisFile in $psAnalysisFiles) {
            try {
              Write-Host "  Processing: $($analysisFile.Name)" -ForegroundColor Gray
              $analysis = Get-Content $analysisFile.FullName | ConvertFrom-Json
              
              # NEW FORMAT: Check Summary.BySeverity structure
              if ($analysis.Summary -and $analysis.Summary.BySeverity) {
                $errorCount = $analysis.Summary.BySeverity.Error ?? 0
                $warningCount = $analysis.Summary.BySeverity.Warning ?? 0
                
                Write-Host "  Errors: $errorCount, Warnings: $warningCount" -ForegroundColor Gray
                
                if ($errorCount -gt 0) {
                  Write-Host "  Found $errorCount PSScriptAnalyzer errors" -ForegroundColor Red
                  $codeQualityIssues += @{
                    Type = "PSScriptAnalyzer"
                    Severity = "Error"
                    Count = $errorCount
                    Violations = @($analysis.Violations | Where-Object { $_.Severity -eq 'Error' })
                  }
                }
                
                if ($warningCount -gt 50) {
                  Write-Host "  Found $warningCount PSScriptAnalyzer warnings (threshold: 50)" -ForegroundColor Yellow
                  $codeQualityIssues += @{
                    Type = "PSScriptAnalyzer"
                    Severity = "Warning"
                    Count = $warningCount
                    Violations = @($analysis.Violations | Where-Object { $_.Severity -eq 'Warning' } | Select-Object -First 50)
                  }
                }
              }
            } catch {
              Write-Warning "Failed to parse analysis: $($analysisFile.Name) - $_"
            }
          }
          
          # Group failures by hash to identify unique issues
          $uniqueFailures = @{}
          foreach ($failure in $testFailures) {
            $hash = $failure.Hash
            if (-not $uniqueFailures.ContainsKey($hash)) {
              $uniqueFailures[$hash] = @{
                Hash = $hash
                TestName = $failure.TestName
                ErrorMessage = $failure.ErrorMessage
                File = $failure.File
                Line = $failure.Line
                TestType = $failure.TestType
                Occurrences = 1
              }
            } else {
              $uniqueFailures[$hash].Occurrences++
            }
          }
          
          # Group failures by file for better issue organization
          $failuresByFile = $uniqueFailures.Values | Group-Object -Property File
          
          # Save analysis results
          $analysisResults = @{
            Timestamp = Get-Date -Format 'yyyy-MM-dd HH:mm:ss'
            TotalFailures = $testFailures.Count
            FailuresByFile = $failuresByFile | ForEach-Object {
              @{
                File = $_.Name
                Count = $_.Count
                Failures = $_.Group
              }
            }
            CodeQualityIssues = $codeQualityIssues
            WorkflowRun = if ($env:GITHUB_EVENT_NAME -eq 'workflow_run') { 
              @{
                Id = "${{ github.event.workflow_run.id }}"
                Conclusion = "${{ github.event.workflow_run.conclusion }}"
                Workflow = "${{ github.event.workflow_run.name }}"
              }
            } else { $null }
          }
          
          $analysisResults | ConvertTo-Json -Depth 10 | Out-File "failure-analysis.json"
          
          # Set outputs
          "has-test-failures=$($testFailures.Count -gt 0)" | Out-File -FilePath $env:GITHUB_OUTPUT -Append
          "has-code-quality-issues=$($codeQualityIssues.Count -gt 0)" | Out-File -FilePath $env:GITHUB_OUTPUT -Append
          "total-failures=$($testFailures.Count)" | Out-File -FilePath $env:GITHUB_OUTPUT -Append
          
          Write-Host "‚úÖ Analysis complete!" -ForegroundColor Green
          Write-Host "  Test Failures: $($testFailures.Count)" -ForegroundColor $(if ($testFailures.Count -gt 0) { 'Red' } else { 'Green' })
          Write-Host "  Code Quality Issues: $($codeQualityIssues.Count)" -ForegroundColor $(if ($codeQualityIssues.Count -gt 0) { 'Yellow' } else { 'Green' })
          
      - name: üì§ Upload Analysis Results
        uses: actions/upload-artifact@v4
        with:
          name: failure-analysis
          path: failure-analysis.json
          retention-days: 30

  create-issues:
    name: üìù Create GitHub Issues
    runs-on: ubuntu-latest
    needs: analyze-test-failures
    if: |
      needs.analyze-test-failures.outputs.has-test-failures == 'true' ||
      needs.analyze-test-failures.outputs.has-code-quality-issues == 'true'
    
    steps:
      - name: üì• Checkout Repository
        uses: actions/checkout@v4
        
      - name: üìä Download Analysis Results
        uses: actions/download-artifact@v4
        with:
          name: failure-analysis
          path: ./
          
      - name: üìù Create Issues from Failures
        if: github.event.inputs.dry_run != 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Helper function to format failures list
            function formatFailuresList(failures, count) {
              const failuresToShow = failures.slice(0, 10);
              const formattedFailures = failuresToShow.map(f => 
                `- **Test:** \`${f.TestName}\`
              - **Error:** ${f.ErrorMessage}
              - **Location:** Line ${f.Line || 'unknown'}`
              ).join('\n\n');
              
              const moreFailures = count > 10 ? `\n*...and ${count - 10} more failures. See full test report for details.*\n` : '';
              
              return formattedFailures + moreFailures;
            }
            
            // Helper function to format workflow context
            function formatWorkflowContext(workflowRun) {
              if (!workflowRun) {
                return '- **Triggered by:** Scheduled analysis';
              }
              
              const workflowRunUrl = `${{ github.server_url }}/${{ github.repository }}/actions/runs/${workflowRun.Id}`;
              return `
            - **Workflow:** ${workflowRun.Workflow}
            - **Run ID:** [${workflowRun.Id}](${workflowRunUrl})
            - **Conclusion:** ${workflowRun.Conclusion}
            `;
            }
            
            // Load analysis results
            const analysisPath = './failure-analysis.json';
            if (!fs.existsSync(analysisPath)) {
              console.log('‚ùå No analysis results found');
              return;
            }
            
            const analysis = JSON.parse(fs.readFileSync(analysisPath, 'utf8'));
            console.log(`üìä Processing ${analysis.TotalFailures} test failures`);
            
            // Get existing automated issues to avoid duplicates
            const { data: existingIssues } = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: 'automated-issue,test-failure',
              state: 'open',
              per_page: 100
            });
            
            console.log(`Found ${existingIssues.length} existing automated test failure issues`);
            
            // Create issues grouped by file
            for (const fileGroup of analysis.FailuresByFile || []) {
              if (!fileGroup.File) continue;
              
              const fileName = fileGroup.File.split('/').pop();
              const issueTitle = `üß™ Test Failures in ${fileName} (${fileGroup.Count} failures)`;
              
              // Check if issue already exists
              const existingIssue = existingIssues.find(i => i.title.includes(fileName));
              
              if (existingIssue) {
                // Update existing issue with latest info
                const updateComment = `## üîÑ Updated Test Failure Report
            
            **Timestamp:** ${analysis.Timestamp}
            **Total Failures:** ${fileGroup.Count}
            
            ### Failed Tests
            ${formatFailuresList(fileGroup.Failures, fileGroup.Count)}
            
            ### Workflow Context
            ${formatWorkflowContext(analysis.WorkflowRun)}
            
            ---
            *This is an automated update. Issue remains open until all tests pass.*`;
                
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: existingIssue.number,
                  body: updateComment
                });
                
                console.log(`‚úÖ Updated existing issue #${existingIssue.number}: ${issueTitle}`);
                continue;
              }
              
              // Create new issue
              const issueBody = `## üß™ Test Failures Detected
            
            **File:** \`${fileGroup.File}\`
            **Total Failures:** ${fileGroup.Count}
            **Detected:** ${analysis.Timestamp}
            
            ### Failed Tests
            ${formatFailuresList(fileGroup.Failures, fileGroup.Count)}
            
            ### ü§ñ AI Agent Instructions
            
            @copilot Please investigate and fix these test failures:
            
            1. **Analyze** the failed tests and identify root causes
            2. **Fix** the underlying issues causing the failures
            3. **Verify** all tests pass after fixes
            4. **Update** this issue with your findings and solutions
            5. **Submit** a PR with the fixes
            
            ### Workflow Context
            ${formatWorkflowContext(analysis.WorkflowRun)}
            
            ---
            *This issue was automatically created from test failure analysis*`;
              
              try {
                const newIssue = await github.rest.issues.create({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  title: issueTitle,
                  body: issueBody,
                  labels: ['automated-issue', 'test-failure', 'p1', 'bug']
                });
                
                console.log(`‚úÖ Created issue #${newIssue.data.number}: ${issueTitle}`);
              } catch (error) {
                console.error(`‚ùå Failed to create issue for ${fileName}: ${error.message}`);
              }
            }
            
            // Create issues for code quality problems
            for (const issue of analysis.CodeQualityIssues || []) {
              const issueTitle = `‚ö†Ô∏è ${issue.Type} ${issue.Severity}: ${issue.Count} issues detected`;
              
              // Check for existing issue
              const existingIssue = existingIssues.find(i => 
                i.title.includes(issue.Type) && i.title.includes(issue.Severity)
              );
              
              if (existingIssue) {
                console.log(`Issue already exists for ${issue.Type} ${issue.Severity}`);
                continue;
              }
              
              const issueBody = `## ‚ö†Ô∏è Code Quality Issues Detected
            
            **Type:** ${issue.Type}
            **Severity:** ${issue.Severity}
            **Count:** ${issue.Count}
            **Detected:** ${analysis.Timestamp}
            
            ### ü§ñ AI Agent Instructions
            
            @copilot Please address these code quality issues:
            
            1. **Review** the PSScriptAnalyzer results in the reports directory
            2. **Prioritize** ${issue.Severity.toLowerCase()} issues by impact
            3. **Fix** the most critical violations first
            4. **Test** to ensure fixes don't break functionality
            5. **Submit** PR(s) with the improvements
            
            ### Context
            ${formatWorkflowContext(analysis.WorkflowRun)}
            
            ---
            *This issue was automatically created from code quality analysis*`;
              
              try {
                const newIssue = await github.rest.issues.create({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  title: issueTitle,
                  body: issueBody,
                  labels: ['automated-issue', 'code-quality', issue.Severity.toLowerCase(), 'p2']
                });
                
                console.log(`‚úÖ Created code quality issue #${newIssue.data.number}`);
              } catch (error) {
                console.error(`‚ùå Failed to create code quality issue: ${error.message}`);
              }
            }
            
            console.log('‚úÖ Issue creation complete!');
            
      - name: üìã Preview Issues (Dry Run)
        if: github.event.inputs.dry_run == 'true'
        shell: pwsh
        run: |
          Write-Host "üß™ DRY RUN MODE - No issues will be created" -ForegroundColor Yellow
          Write-Host ""
          
          $analysis = Get-Content "./failure-analysis.json" | ConvertFrom-Json
          
          Write-Host "üìä Issues that would be created:" -ForegroundColor Cyan
          Write-Host "================================" -ForegroundColor Cyan
          
          foreach ($fileGroup in $analysis.FailuresByFile) {
            Write-Host ""
            Write-Host "üß™ Test Failure Issue:" -ForegroundColor Red
            Write-Host "  File: $($fileGroup.File)"
            Write-Host "  Failures: $($fileGroup.Count)"
          }
          
          foreach ($issue in $analysis.CodeQualityIssues) {
            Write-Host ""
            Write-Host "‚ö†Ô∏è Code Quality Issue:" -ForegroundColor Yellow
            Write-Host "  Type: $($issue.Type)"
            Write-Host "  Severity: $($issue.Severity)"
            Write-Host "  Count: $($issue.Count)"
          }
          
          Write-Host ""
          Write-Host "üí° Run without dry_run to create these issues" -ForegroundColor Blue
